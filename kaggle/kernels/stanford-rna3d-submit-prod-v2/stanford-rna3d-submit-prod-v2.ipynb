{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e18240b",
   "metadata": {},
   "source": "# Stanford RNA3D submit notebook (Fase 1 + Fase 2 full pipeline)\n\nPipeline completo com contratos estritos (fail-fast), sem fallback silencioso."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0504bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import hashlib\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "SCRIPT_LOC = \"submission_notebook_phase1_phase2_full_v2\"\n",
    "N_MODELS = 5\n",
    "TOP_K = 16\n",
    "TEMPLATE_SCORE_THRESHOLD = 0.65\n",
    "LEN_THRESHOLD = 68\n",
    "TBM_CHUNK_SIZE = 12\n",
    "\n",
    "\n",
    "def _die(stage: str, where: str, cause: str, impact, examples) -> None:\n",
    "    examples_text = \",\".join(str(x) for x in examples) if examples else \"-\"\n",
    "    raise RuntimeError(f\"[{stage}] [{where}] {cause} | impacto={impact} | exemplos={examples_text}\")\n",
    "\n",
    "\n",
    "def _tail(stdout: str | None, stderr: str | None, n: int = 20) -> list[str]:\n",
    "    text = ((stdout or \"\") + \"\\n\" + (stderr or \"\")).strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    return text.splitlines()[-n:]\n",
    "\n",
    "\n",
    "def _run(cmd: list[str], env: dict[str, str], cwd: Path) -> None:\n",
    "    where = f\"{SCRIPT_LOC}:run\"\n",
    "    print(\"[RUN]\", \" \".join(cmd))\n",
    "    proc = subprocess.run(cmd, env=env, cwd=str(cwd), text=True, capture_output=True)\n",
    "    if proc.returncode != 0:\n",
    "        _die(\"PIPELINE\", where, \"comando falhou\", proc.returncode, _tail(proc.stdout, proc.stderr, 20))\n",
    "    out = (proc.stdout or \"\").strip()\n",
    "    if out:\n",
    "        print(out)\n",
    "\n",
    "\n",
    "def _collect_datasets(input_root: Path) -> list[Path]:\n",
    "    where = f\"{SCRIPT_LOC}:collect_datasets\"\n",
    "    if not input_root.exists():\n",
    "        _die(\"LOAD\", where, \"diretorio /kaggle/input ausente\", 1, [str(input_root)])\n",
    "    datasets = [path for path in sorted(input_root.iterdir()) if path.is_dir()]\n",
    "    if not datasets:\n",
    "        _die(\"LOAD\", where, \"nenhum dataset montado em /kaggle/input\", 0, [])\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def _find_by_filename(datasets: list[Path], filename: str) -> list[Path]:\n",
    "    hits: list[Path] = []\n",
    "    for ds in datasets:\n",
    "        hits.extend([path for path in ds.rglob(filename) if path.is_file()])\n",
    "    return hits\n",
    "\n",
    "\n",
    "def _find_by_pattern(datasets: list[Path], pattern: str) -> list[Path]:\n",
    "    hits: list[Path] = []\n",
    "    for ds in datasets:\n",
    "        hits.extend([path for path in ds.rglob(pattern) if path.is_file()])\n",
    "    return hits\n",
    "\n",
    "\n",
    "def _require_single(label: str, candidates: list[Path], *, stage: str, where: str) -> Path:\n",
    "    unique: list[Path] = []\n",
    "    seen: set[str] = set()\n",
    "    for item in candidates:\n",
    "        key = str(item.resolve())\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique.append(item.resolve())\n",
    "    if not unique:\n",
    "        _die(stage, where, f\"ativo obrigatorio ausente: {label}\", 1, [label])\n",
    "    if len(unique) > 1:\n",
    "        _die(stage, where, f\"ativo ambiguo para {label}\", len(unique), [str(path) for path in unique[:8]])\n",
    "    return unique[0]\n",
    "\n",
    "\n",
    "def _require_any(label: str, candidates: list[Path], *, stage: str, where: str) -> list[Path]:\n",
    "    unique: list[Path] = []\n",
    "    seen: set[str] = set()\n",
    "    for item in candidates:\n",
    "        key = str(item.resolve())\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique.append(item.resolve())\n",
    "    if not unique:\n",
    "        _die(stage, where, f\"ativo obrigatorio ausente: {label}\", 1, [label])\n",
    "    return unique\n",
    "\n",
    "\n",
    "def _src_supports_command(src_root: Path, command: str) -> bool:\n",
    "    env = os.environ.copy()\n",
    "    env[\"PYTHONPATH\"] = str(src_root) + ((\":\" + env[\"PYTHONPATH\"]) if env.get(\"PYTHONPATH\") else \"\")\n",
    "    proc = subprocess.run([sys.executable, \"-m\", \"rna3d_local\", command, \"--help\"], env=env, cwd=str(src_root.parent), text=True, capture_output=True)\n",
    "    return proc.returncode == 0\n",
    "\n",
    "\n",
    "def _find_src_root(datasets: list[Path], unpack_root: Path) -> Path:\n",
    "    where = f\"{SCRIPT_LOC}:find_src_root\"\n",
    "    candidates: list[Path] = [ds / \"src\" for ds in datasets if (ds / \"src\" / \"rna3d_local\" / \"cli.py\").exists()]\n",
    "\n",
    "    zip_candidates = _find_by_filename(datasets, \"src_reboot.zip\")\n",
    "    if zip_candidates:\n",
    "        zip_path = _require_single(\"src_reboot.zip\", zip_candidates, stage=\"LOAD\", where=where)\n",
    "        if unpack_root.exists() and any(unpack_root.iterdir()):\n",
    "            _die(\"LOAD\", where, \"diretorio de unpack do src nao-vazio\", 1, [str(unpack_root)])\n",
    "        unpack_root.mkdir(parents=True, exist_ok=True)\n",
    "        with zipfile.ZipFile(str(zip_path), \"r\") as archive:\n",
    "            archive.extractall(str(unpack_root))\n",
    "        extracted_src = unpack_root / \"src\"\n",
    "        if not (extracted_src / \"rna3d_local\" / \"cli.py\").exists():\n",
    "            _die(\"LOAD\", where, \"src_reboot.zip sem src/rna3d_local/cli.py\", 1, [str(zip_path)])\n",
    "        candidates.append(extracted_src)\n",
    "\n",
    "    if not candidates:\n",
    "        _die(\"LOAD\", where, \"nenhum src/rna3d_local/cli.py encontrado\", 1, [])\n",
    "\n",
    "    unique: list[Path] = []\n",
    "    seen: set[str] = set()\n",
    "    for item in candidates:\n",
    "        key = str(item.resolve())\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique.append(item.resolve())\n",
    "\n",
    "    compatible = [src for src in unique if _src_supports_command(src, \"build-embedding-index\")]\n",
    "    if not compatible:\n",
    "        _die(\"LOAD\", where, \"nenhum src compativel com CLI da Fase 1+2\", len(unique), [str(path) for path in unique[:8]])\n",
    "    if len(compatible) > 1:\n",
    "        _die(\"LOAD\", where, \"multiplos src compativeis; selecao ambigua\", len(compatible), [str(path) for path in compatible[:8]])\n",
    "    return compatible[0]\n",
    "\n",
    "\n",
    "def _find_template_assets(datasets: list[Path]) -> tuple[Path, Path]:\n",
    "    where = f\"{SCRIPT_LOC}:find_template_assets\"\n",
    "\n",
    "    # Prefer deterministic sources to avoid ambiguity when multiple datasets ship template_db.\n",
    "    preferred = [\n",
    "        \"stanford-rna3d-reboot-src-v2\",\n",
    "        \"ribonanza-quickstart-3d-templates\",\n",
    "    ]\n",
    "    for ds_name in preferred:\n",
    "        ds = next((item for item in datasets if item.name == ds_name), None)\n",
    "        if ds is None:\n",
    "            continue\n",
    "        idx_candidates = _find_by_filename([ds], \"template_index.parquet\")\n",
    "        tpl_candidates = _find_by_filename([ds], \"templates.parquet\")\n",
    "        if idx_candidates and tpl_candidates:\n",
    "            template_index = _require_single(\"template_index.parquet\", idx_candidates, stage=\"LOAD\", where=where)\n",
    "            templates = _require_single(\"templates.parquet\", tpl_candidates, stage=\"LOAD\", where=where)\n",
    "            return template_index, templates\n",
    "\n",
    "    template_index = _require_single(\"template_index.parquet\", _find_by_filename(datasets, \"template_index.parquet\"), stage=\"LOAD\", where=where)\n",
    "    templates = _require_single(\"templates.parquet\", _find_by_filename(datasets, \"templates.parquet\"), stage=\"LOAD\", where=where)\n",
    "    return template_index, templates\n",
    "\n",
    "\n",
    "def _find_quickstart_file(datasets: list[Path]) -> Path:\n",
    "    where = f\"{SCRIPT_LOC}:find_quickstart\"\n",
    "    candidates = _find_by_pattern(datasets, \"*QUICK_START*.csv\") + _find_by_pattern(datasets, \"*quickstart*.csv\") + _find_by_pattern(datasets, \"*quickstart*.parquet\")\n",
    "    return _require_single(\"quickstart\", candidates, stage=\"LOAD\", where=where)\n",
    "\n",
    "\n",
    "def _materialize_phase2_assets(datasets: list[Path], dst: Path) -> Path:\n",
    "    where = f\"{SCRIPT_LOC}:materialize_phase2_assets\"\n",
    "    # Locate a phase2 assets root that matches the rna3d_local offline runners contract.\n",
    "    # Expected layout: <root>/models/{rnapro,chai1,boltz1}/...\n",
    "    required_markers = [\n",
    "        (\"rnapro\", Path(\"models/rnapro/rnapro-public-best-500m.ckpt\")),\n",
    "        (\"chai1\", Path(\"models/chai1/models_v2/trunk.pt\")),\n",
    "        (\"boltz1\", Path(\"models/boltz1/boltz1_conf.ckpt\")),\n",
    "    ]\n",
    "    roots: list[Path] = []\n",
    "    for ds in datasets:\n",
    "        for cand in [\n",
    "            ds,\n",
    "            ds / \"export\" / \"kaggle_assets\",\n",
    "            ds / \"kaggle_assets\",\n",
    "            ds / \"export\" / \"kaggle_assets\" / \"export\" / \"kaggle_assets\",\n",
    "        ]:\n",
    "            ok = True\n",
    "            for _label, rel in required_markers:\n",
    "                if not (cand / rel).exists():\n",
    "                    ok = False\n",
    "                    break\n",
    "            if ok:\n",
    "                roots.append(cand.resolve())\n",
    "    uniq: list[Path] = []\n",
    "    seen: set[str] = set()\n",
    "    for r in roots:\n",
    "        k = str(r)\n",
    "        if k not in seen:\n",
    "            seen.add(k)\n",
    "            uniq.append(r)\n",
    "    if not uniq:\n",
    "        _die(\"LOAD\", where, \"assets phase2 nao encontrados (markers ausentes)\", 1, [str(m[1]) for m in required_markers])\n",
    "    if len(uniq) > 1:\n",
    "        _die(\"LOAD\", where, \"assets phase2 ambiguos (multiplos roots)\", len(uniq), [str(p) for p in uniq[:8]])\n",
    "    return uniq[0]\n",
    "\n",
    "def _build_template_family_map(template_index_path: Path, out_path: Path) -> Path:\n",
    "    where = f\"{SCRIPT_LOC}:build_template_family_map\"\n",
    "    df = pl.read_parquet(str(template_index_path))\n",
    "    if \"template_uid\" not in df.columns:\n",
    "        _die(\"PIPELINE\", where, \"template_index sem template_uid\", 1, [str(template_index_path)])\n",
    "    out = df.select(pl.col(\"template_uid\").cast(pl.Utf8).unique().sort()).with_columns(pl.lit(\"unknown\").alias(\"family_label\"))\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    out.write_parquet(str(out_path), compression=\"zstd\")\n",
    "    if out.height == 0:\n",
    "        _die(\"PIPELINE\", where, \"template_family_map vazio\", 0, [str(out_path)])\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def _build_weak_labels(candidates_path: Path, out_path: Path) -> Path:\n",
    "    where = f\"{SCRIPT_LOC}:build_weak_labels\"\n",
    "    df = pl.read_parquet(str(candidates_path))\n",
    "    required = [\"target_id\", \"template_uid\"]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        _die(\"PIPELINE\", where, \"candidates sem colunas obrigatorias\", len(missing), missing)\n",
    "\n",
    "    score_col = None\n",
    "    for c in [\"final_score\", \"cosine_score\", \"rank\"]:\n",
    "        if c in df.columns:\n",
    "            score_col = c\n",
    "            break\n",
    "    if score_col is None:\n",
    "        _die(\"PIPELINE\", where, \"candidates sem coluna de ranking/score\", 1, [str(df.columns[:8])])\n",
    "\n",
    "    if score_col == \"rank\":\n",
    "        ordered = df.sort([\"target_id\", \"rank\"], descending=[False, False])\n",
    "    else:\n",
    "        ordered = df.sort([\"target_id\", score_col], descending=[False, True])\n",
    "\n",
    "    top1 = ordered.group_by(\"target_id\").agg(pl.first(\"template_uid\").alias(\"top_template\"))\n",
    "    labels = (\n",
    "        ordered.select(\"target_id\", \"template_uid\")\n",
    "        .unique()\n",
    "        .join(top1, on=\"target_id\", how=\"left\")\n",
    "        .with_columns((pl.col(\"template_uid\") == pl.col(\"top_template\")).cast(pl.Float64).alias(\"label\"))\n",
    "        .select(\"target_id\", \"template_uid\", \"label\")\n",
    "    )\n",
    "    if labels.height < 8:\n",
    "        _die(\"PIPELINE\", where, \"labels fracos insuficientes\", labels.height, [\"min=8\"]) \n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    labels.write_parquet(str(out_path), compression=\"zstd\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def _assert_cli_commands(env: dict[str, str], repo_root: Path) -> None:\n",
    "    where = f\"{SCRIPT_LOC}:assert_cli\"\n",
    "    required = [\n",
    "        \"build-embedding-index\",\n",
    "        \"infer-description-family\",\n",
    "        \"retrieve-templates-latent\",\n",
    "        \"predict-tbm\",\n",
    "        \"predict-rnapro-offline\",\n",
    "        \"export-submission\",\n",
    "        \"check-submission\",\n",
    "    ]\n",
    "    for command in required:\n",
    "        proc = subprocess.run([sys.executable, \"-m\", \"rna3d_local\", command, \"--help\"], env=env, cwd=str(repo_root), text=True, capture_output=True)\n",
    "        if proc.returncode != 0:\n",
    "            _die(\"ENV\", where, f\"comando ausente no pacote rna3d_local: {command}\", proc.returncode, _tail(proc.stdout, proc.stderr, 12))\n",
    "\n",
    "\n",
    "import uuid\n",
    "\n",
    "comp_root = Path(\"/kaggle/input/stanford-rna-3d-folding-2\")\n",
    "input_root = Path(\"/kaggle/input\")\n",
    "work_root = Path(\"/kaggle/working\")\n",
    "run_root = work_root / f\"run_{SCRIPT_LOC}_{uuid.uuid4().hex[:8]}\"\n",
    "submission_path = work_root / \"submission.csv\"\n",
    "\n",
    "sample_path = comp_root / \"sample_submission.csv\"\n",
    "targets_path = comp_root / \"test_sequences.csv\"\n",
    "if not sample_path.exists():\n",
    "    _die(\"LOAD\", f\"{SCRIPT_LOC}:paths\", \"sample_submission.csv ausente\", 1, [str(sample_path)])\n",
    "if not targets_path.exists():\n",
    "    _die(\"LOAD\", f\"{SCRIPT_LOC}:paths\", \"test_sequences.csv ausente\", 1, [str(targets_path)])\n",
    "\n",
    "run_root.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "datasets = _collect_datasets(input_root)\n",
    "src_root = _find_src_root(datasets, run_root / \"src_unpack\")\n",
    "template_index_path, templates_path = _find_template_assets(datasets)\n",
    "template_family_map_path = _build_template_family_map(template_index_path, run_root / \"template_family_map.parquet\")\n",
    "\n",
    "repo_root = src_root.parent\n",
    "if str(src_root) not in sys.path:\n",
    "    sys.path.insert(0, str(src_root))\n",
    "\n",
    "env = os.environ.copy()\n",
    "env[\"PYTHONPATH\"] = str(src_root) + ((\":\" + env[\"PYTHONPATH\"]) if env.get(\"PYTHONPATH\") else \"\")\n",
    "env.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "env.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "env.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "env.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "env.setdefault(\"POLARS_MAX_THREADS\", \"1\")\n",
    "# Avoid export streaming partition path conflicts in Kaggle runtime; we already keep RAM low upstream.\n",
    "env[\"RNA3D_EXPORT_STREAMING_THRESHOLD_BYTES\"] = \"1000000000000\"\n",
    "\n",
    "_assert_cli_commands(env, repo_root)\n",
    "\n",
    "print(f\"[INFO] [{SCRIPT_LOC}] run_root={run_root}\")\n",
    "print(f\"[INFO] [{SCRIPT_LOC}] src_root={src_root}\")\n",
    "print(f\"[INFO] [{SCRIPT_LOC}] template_index={template_index_path}\")\n",
    "print(f\"[INFO] [{SCRIPT_LOC}] templates={templates_path}\")\n",
    "\n",
    "emb_dir = run_root / \"embedding\"\n",
    "emb_path = emb_dir / \"template_embeddings.parquet\"\n",
    "retrieval_path = run_root / \"retrieval_candidates.parquet\"\n",
    "retrieval_tbm_path = run_root / \"retrieval_candidates_tbm.parquet\"\n",
    "targets_tbm_path = run_root / \"targets_tbm.csv\"\n",
    "targets_fallback_path = run_root / \"targets_fallback.csv\"\n",
    "tbm_path = run_root / \"tbm_predictions.parquet\"\n",
    "drfold_path = run_root / \"drfold_predictions.parquet\"\n",
    "combined_path = run_root / \"combined_predictions.parquet\"\n",
    "\n",
    "_run([sys.executable, \"-m\", \"rna3d_local\", \"build-embedding-index\", \"--template-index\", str(template_index_path), \"--out-dir\", str(emb_dir), \"--embedding-dim\", \"256\", \"--encoder\", \"mock\", \"--ann-engine\", \"none\"], env, repo_root)\n",
    "_run([\n",
    "    sys.executable, \"-m\", \"rna3d_local\", \"retrieve-templates-latent\",\n",
    "    \"--template-index\", str(template_index_path),\n",
    "    \"--template-embeddings\", str(emb_path),\n",
    "    \"--targets\", str(targets_path),\n",
    "    \"--out\", str(retrieval_path),\n",
    "    \"--top-k\", str(TOP_K),\n",
    "    \"--encoder\", \"mock\",\n",
    "    \"--embedding-dim\", \"256\",\n",
    "    \"--ann-engine\", \"numpy_bruteforce\",\n",
    "    \"--weight-embed\", \"0.90\",\n",
    "    \"--weight-llm\", \"0.00\",\n",
    "    \"--weight-seq\", \"0.10\",\n",
    "], env, repo_root)\n",
    "\n",
    "# Compute which targets can be covered by at least one contiguous template with length >= target_len.\n",
    "targets_df = pl.read_csv(targets_path)\n",
    "if \"target_id\" not in targets_df.columns or \"sequence\" not in targets_df.columns:\n",
    "    _die(\"LOAD\", f\"{SCRIPT_LOC}:targets\", \"targets sem colunas target_id/sequence\", 1, [str(targets_df.columns)])\n",
    "targets_df = targets_df.with_columns(pl.col(\"target_id\").cast(pl.Utf8), pl.col(\"sequence\").cast(pl.Utf8))\n",
    "targets_len = targets_df.select(pl.col(\"target_id\"), pl.col(\"sequence\").str.replace_all(r\"\\|\", \"\").str.len_chars().alias(\"target_len\"))\n",
    "\n",
    "tpl_stats = pl.scan_parquet(str(templates_path)).select(pl.col(\"template_uid\").cast(pl.Utf8), pl.col(\"resid\").cast(pl.Int32))\n",
    "tpl_stats = tpl_stats.group_by(\"template_uid\").agg(pl.col(\"resid\").min().alias(\"min_resid\"), pl.col(\"resid\").max().alias(\"max_resid\"), pl.col(\"resid\").n_unique().alias(\"n_unique\"))\n",
    "tpl_stats = tpl_stats.with_columns((pl.col(\"max_resid\") - pl.col(\"min_resid\") + 1).cast(pl.Int32).alias(\"tpl_len\"))\n",
    "tpl_stats = tpl_stats.with_columns((pl.col(\"n_unique\") == pl.col(\"tpl_len\")).alias(\"contiguous\"))\n",
    "\n",
    "retr_lf = pl.scan_parquet(str(retrieval_path)).select(pl.col(\"target_id\").cast(pl.Utf8), pl.col(\"template_uid\").cast(pl.Utf8)).unique()\n",
    "supported = retr_lf.join(targets_len.lazy(), on=\"target_id\", how=\"inner\").join(tpl_stats, on=\"template_uid\", how=\"inner\")\n",
    "supported = supported.filter(pl.col(\"contiguous\") & (pl.col(\"tpl_len\") >= pl.col(\"target_len\"))).select(pl.col(\"target_id\")).unique().collect()\n",
    "supported_ids = set(supported.get_column(\"target_id\").to_list())\n",
    "all_ids = set(targets_len.get_column(\"target_id\").to_list())\n",
    "fallback_ids = sorted(all_ids - supported_ids)\n",
    "print(f\"[INFO] [{SCRIPT_LOC}] tbm_supported={len(supported_ids)} fallback={len(fallback_ids)}\")\n",
    "if len(supported_ids) == 0:\n",
    "    _die(\"PIPELINE\", f\"{SCRIPT_LOC}:coverage\", \"nenhum alvo com template valido para TBM\", 0, [])\n",
    "\n",
    "# Write fallback targets once.\n",
    "targets_df.filter(pl.col(\"target_id\").is_in(fallback_ids)).write_csv(targets_fallback_path)\n",
    "\n",
    "# Run TBM in small chunks to cap peak RAM in hidden reruns.\n",
    "supported_ids_sorted = sorted(supported_ids)\n",
    "tbm_parts_dir = run_root / \"tbm_parts\"\n",
    "tbm_parts_dir.mkdir(parents=True, exist_ok=True)\n",
    "tbm_part_paths: list[Path] = []\n",
    "for chunk_start in range(0, len(supported_ids_sorted), int(TBM_CHUNK_SIZE)):\n",
    "    chunk_ids = supported_ids_sorted[chunk_start : chunk_start + int(TBM_CHUNK_SIZE)]\n",
    "    if not chunk_ids:\n",
    "        continue\n",
    "    chunk_tag = f\"{chunk_start // int(TBM_CHUNK_SIZE):04d}\"\n",
    "    targets_chunk_path = tbm_parts_dir / f\"targets_tbm_{chunk_tag}.csv\"\n",
    "    retrieval_chunk_path = tbm_parts_dir / f\"retrieval_tbm_{chunk_tag}.parquet\"\n",
    "    tbm_chunk_path = tbm_parts_dir / f\"tbm_predictions_{chunk_tag}.parquet\"\n",
    "\n",
    "    targets_df.filter(pl.col(\"target_id\").is_in(chunk_ids)).write_csv(targets_chunk_path)\n",
    "    pl.scan_parquet(str(retrieval_path)).filter(pl.col(\"target_id\").is_in(chunk_ids)).sink_parquet(str(retrieval_chunk_path), engine=\"streaming\")\n",
    "\n",
    "    _run([\n",
    "        sys.executable,\n",
    "        \"-m\",\n",
    "        \"rna3d_local\",\n",
    "        \"predict-tbm\",\n",
    "        \"--retrieval\",\n",
    "        str(retrieval_chunk_path),\n",
    "        \"--templates\",\n",
    "        str(templates_path),\n",
    "        \"--targets\",\n",
    "        str(targets_chunk_path),\n",
    "        \"--out\",\n",
    "        str(tbm_chunk_path),\n",
    "        \"--n-models\",\n",
    "        str(N_MODELS),\n",
    "    ], env, repo_root)\n",
    "    if not tbm_chunk_path.exists():\n",
    "        _die(\"PIPELINE\", f\"{SCRIPT_LOC}:tbm_chunk\", \"predict-tbm sem saida no chunk\", 1, [str(tbm_chunk_path)])\n",
    "    tbm_part_paths.append(tbm_chunk_path)\n",
    "\n",
    "if not tbm_part_paths:\n",
    "    _die(\"PIPELINE\", f\"{SCRIPT_LOC}:tbm_chunk\", \"nenhum chunk TBM gerado\", 0, [])\n",
    "\n",
    "pl.scan_parquet([str(p) for p in tbm_part_paths]).sink_parquet(str(tbm_path), engine=\"streaming\")\n",
    "\n",
    "def _normalize_seq(seq: str) -> str:\n",
    "    raw = str(seq or \"\").strip().upper().replace(\"T\", \"U\")\n",
    "    return \"\".join(ch for ch in raw if ch not in {\"|\", \" \", \"\\t\", \"\\n\", \"\\r\"})\n",
    "\n",
    "\n",
    "def _assert_target_model_coverage(predictions_path: Path, target_id: str, *, n_models: int, target_len: int) -> None:\n",
    "    where = f\"{SCRIPT_LOC}:fallback_coverage\"\n",
    "    if target_len <= 0:\n",
    "        _die(\"FALLBACK\", where, \"target_len invalido\", 1, [f\"{target_id}:{target_len}\"])\n",
    "    df = (\n",
    "        pl.scan_parquet(str(predictions_path))\n",
    "        .filter(pl.col(\"target_id\") == target_id)\n",
    "        .select(\n",
    "            pl.col(\"model_id\").cast(pl.Int32),\n",
    "            pl.col(\"resid\").cast(pl.Int32),\n",
    "        )\n",
    "        .collect(streaming=True)\n",
    "    )\n",
    "    if df.height <= 0:\n",
    "        _die(\"FALLBACK\", where, \"fallback sem linhas para target\", 1, [target_id, str(predictions_path)])\n",
    "\n",
    "    stats = (\n",
    "        df.group_by(\"model_id\")\n",
    "        .agg(\n",
    "            pl.col(\"resid\").n_unique().alias(\"n_unique\"),\n",
    "            pl.col(\"resid\").min().alias(\"min_resid\"),\n",
    "            pl.col(\"resid\").max().alias(\"max_resid\"),\n",
    "        )\n",
    "        .sort(\"model_id\")\n",
    "    )\n",
    "    mids = sorted(stats.get_column(\"model_id\").to_list())\n",
    "    expected_mids = list(range(1, int(n_models) + 1))\n",
    "    if mids != expected_mids:\n",
    "        _die(\"FALLBACK\", where, \"fallback sem model_id esperado\", 1, [f\"{target_id}:got={mids}:expected={expected_mids}\"])\n",
    "\n",
    "    bad = stats.filter(\n",
    "        (pl.col(\"n_unique\") != int(target_len))\n",
    "        | (pl.col(\"min_resid\") != 1)\n",
    "        | (pl.col(\"max_resid\") != int(target_len))\n",
    "    )\n",
    "    if bad.height > 0:\n",
    "        examples = (\n",
    "            bad.select(\n",
    "                (\n",
    "                    pl.col(\"model_id\").cast(pl.Utf8)\n",
    "                    + pl.lit(\":\")\n",
    "                    + pl.col(\"n_unique\").cast(pl.Utf8)\n",
    "                    + pl.lit(\":\")\n",
    "                    + pl.col(\"min_resid\").cast(pl.Utf8)\n",
    "                    + pl.lit(\":\")\n",
    "                    + pl.col(\"max_resid\").cast(pl.Utf8)\n",
    "                ).alias(\"k\")\n",
    "            )\n",
    "            .head(8)\n",
    "            .get_column(\"k\")\n",
    "            .to_list()\n",
    "        )\n",
    "        _die(\"FALLBACK\", where, \"fallback com cobertura insuficiente\", int(bad.height), [f\"{target_id}:{x}\" for x in examples])\n",
    "\n",
    "\n",
    "# Phase2 fallback for targets sem cobertura TBM estrita.\n",
    "if fallback_ids:\n",
    "    phase2_assets_root = _materialize_phase2_assets(datasets, run_root / \"phase2_assets\")\n",
    "    rnapro_model_dir = phase2_assets_root / \"models\" / \"rnapro\"\n",
    "    if not rnapro_model_dir.exists():\n",
    "        _die(\"FALLBACK\", f\"{SCRIPT_LOC}:rnapro\", \"model dir RNApro ausente\", 1, [str(rnapro_model_dir)])\n",
    "\n",
    "    fallback_targets_df = targets_df.filter(pl.col(\"target_id\").is_in(fallback_ids)).sort(\"target_id\")\n",
    "    fallback_targets_df.write_csv(targets_fallback_path)\n",
    "\n",
    "    fallback_pred_path = run_root / \"fallback_rnapro_predictions.parquet\"\n",
    "    _run([\n",
    "        sys.executable,\n",
    "        \"-m\",\n",
    "        \"rna3d_local\",\n",
    "        \"predict-rnapro-offline\",\n",
    "        \"--model-dir\",\n",
    "        str(rnapro_model_dir),\n",
    "        \"--targets\",\n",
    "        str(targets_fallback_path),\n",
    "        \"--out\",\n",
    "        str(fallback_pred_path),\n",
    "        \"--n-models\",\n",
    "        str(N_MODELS),\n",
    "    ], env, repo_root)\n",
    "\n",
    "    for tid in fallback_ids:\n",
    "        seq_raw = targets_df.filter(pl.col(\"target_id\") == tid).get_column(\"sequence\").item()\n",
    "        seq = _normalize_seq(seq_raw)\n",
    "        _assert_target_model_coverage(fallback_pred_path, tid, n_models=int(N_MODELS), target_len=len(seq))\n",
    "\n",
    "    pl.concat([pl.scan_parquet(str(tbm_path)), pl.scan_parquet(str(fallback_pred_path))], how=\"vertical_relaxed\").sink_parquet(str(combined_path), engine=\"streaming\")\n",
    "else:\n",
    "    pl.scan_parquet(str(tbm_path)).sink_parquet(str(combined_path), engine=\"streaming\")\n",
    "\n",
    "\n",
    "def _center_predictions_long(predictions_path: Path, out_path: Path) -> None:\n",
    "    where = f\"{SCRIPT_LOC}:center_predictions\"\n",
    "    lf = pl.scan_parquet(str(predictions_path))\n",
    "    required = {\"target_id\", \"model_id\", \"resid\", \"resname\", \"x\", \"y\", \"z\"}\n",
    "    cols = set(lf.collect_schema().names())\n",
    "    missing = sorted(required - cols)\n",
    "    if missing:\n",
    "        _die(\"EXPORT\", where, \"predictions sem colunas obrigatorias para centralizacao\", len(missing), missing)\n",
    "\n",
    "    means = lf.group_by([\"target_id\", \"model_id\"]).agg(\n",
    "        pl.col(\"x\").cast(pl.Float64).mean().alias(\"_mx\"),\n",
    "        pl.col(\"y\").cast(pl.Float64).mean().alias(\"_my\"),\n",
    "        pl.col(\"z\").cast(pl.Float64).mean().alias(\"_mz\"),\n",
    "    )\n",
    "    centered = (\n",
    "        lf.join(means, on=[\"target_id\", \"model_id\"], how=\"left\")\n",
    "        .with_columns(\n",
    "            (pl.col(\"x\").cast(pl.Float64) - pl.col(\"_mx\")).alias(\"x\"),\n",
    "            (pl.col(\"y\").cast(pl.Float64) - pl.col(\"_my\")).alias(\"y\"),\n",
    "            (pl.col(\"z\").cast(pl.Float64) - pl.col(\"_mz\")).alias(\"z\"),\n",
    "        )\n",
    "        .select(\n",
    "            pl.col(\"target_id\").cast(pl.Utf8),\n",
    "            pl.col(\"model_id\").cast(pl.Int32),\n",
    "            pl.col(\"resid\").cast(pl.Int32),\n",
    "            pl.col(\"resname\").cast(pl.Utf8),\n",
    "            pl.col(\"x\").cast(pl.Float64),\n",
    "            pl.col(\"y\").cast(pl.Float64),\n",
    "            pl.col(\"z\").cast(pl.Float64),\n",
    "        )\n",
    "    )\n",
    "    centered.sink_parquet(str(out_path), engine=\"streaming\")\n",
    "\n",
    "\n",
    "def _assert_submission_coord_bounds(sub_path: Path, *, abs_max: float = 1000.0) -> None:\n",
    "    where = f\"{SCRIPT_LOC}:submission_bounds\"\n",
    "    if abs_max <= 0:\n",
    "        _die(\"CHECK\", where, \"abs_max invalido\", 1, [str(abs_max)])\n",
    "    with sub_path.open(\"r\", encoding=\"utf-8\", newline=\"\") as handle:\n",
    "        reader = csv.reader(handle)\n",
    "        header = next(reader, None)\n",
    "        if not header:\n",
    "            _die(\"CHECK\", where, \"submission csv vazio\", 1, [str(sub_path)])\n",
    "        coord_idxs = [idx for idx, name in enumerate(header) if name.startswith((\"x_\", \"y_\", \"z_\"))]\n",
    "        bad: list[str] = []\n",
    "        for row_idx, row in enumerate(reader, start=1):\n",
    "            if len(row) != len(header):\n",
    "                _die(\"CHECK\", where, \"linha com numero de colunas invalido\", 1, [f\"row={row_idx}\"])\n",
    "            for cidx in coord_idxs:\n",
    "                col = header[cidx]\n",
    "                raw = row[cidx]\n",
    "                try:\n",
    "                    val = float(raw)\n",
    "                except Exception:\n",
    "                    bad.append(f\"{col}@{row_idx}:non-numeric\")\n",
    "                    if len(bad) >= 8:\n",
    "                        _die(\"CHECK\", where, \"coordenadas invalidas na submission\", len(bad), bad)\n",
    "                    continue\n",
    "                if not (val == val) or abs(val) > abs_max:\n",
    "                    bad.append(f\"{col}@{row_idx}:abs>{abs_max:g}\")\n",
    "                    if len(bad) >= 8:\n",
    "                        _die(\"CHECK\", where, \"coordenadas invalidas na submission\", len(bad), bad)\n",
    "        if bad:\n",
    "            _die(\"CHECK\", where, \"coordenadas invalidas na submission\", len(bad), bad)\n",
    "\n",
    "\n",
    "def _parse_target_resid(id_value: str) -> tuple[str, int]:\n",
    "    where = f\"{SCRIPT_LOC}:parse_id\"\n",
    "    key = str(id_value or \"\")\n",
    "    if \"_\" not in key:\n",
    "        _die(\"EXPORT\", where, \"ID invalido (esperado <target>_<resid>)\", 1, [key])\n",
    "    target_id, resid_str = key.rsplit(\"_\", 1)\n",
    "    try:\n",
    "        resid = int(resid_str)\n",
    "    except Exception:\n",
    "        _die(\"EXPORT\", where, \"ID invalido (resid nao-inteiro)\", 1, [key])\n",
    "    return target_id, resid\n",
    "\n",
    "\n",
    "def _model_ids_from_sample_header(header: list[str]) -> list[int]:\n",
    "    where = f\"{SCRIPT_LOC}:sample_header\"\n",
    "    mids = sorted(int(c.split(\"_\", 1)[1]) for c in header if c.startswith(\"x_\"))\n",
    "    if not mids:\n",
    "        _die(\"EXPORT\", where, \"sample sem colunas de modelo\", 1, header[:8])\n",
    "    for mid in mids:\n",
    "        for pref in (\"y_\", \"z_\"):\n",
    "            col = f\"{pref}{mid}\"\n",
    "            if col not in header:\n",
    "                _die(\"EXPORT\", where, \"sample sem coluna obrigatoria de modelo\", 1, [col])\n",
    "    return mids\n",
    "\n",
    "\n",
    "def _load_target_coords(predictions_path: Path, target_id: str, model_ids: list[int]) -> dict[int, dict[int, tuple[float, float, float]]]:\n",
    "    where = f\"{SCRIPT_LOC}:load_target_coords\"\n",
    "    df = (\n",
    "        pl.scan_parquet(str(predictions_path))\n",
    "        .filter(pl.col(\"target_id\") == target_id)\n",
    "        .select(\n",
    "            pl.col(\"model_id\").cast(pl.Int32),\n",
    "            pl.col(\"resid\").cast(pl.Int32),\n",
    "            pl.col(\"x\").cast(pl.Float64),\n",
    "            pl.col(\"y\").cast(pl.Float64),\n",
    "            pl.col(\"z\").cast(pl.Float64),\n",
    "        )\n",
    "        .collect(streaming=True)\n",
    "    )\n",
    "    if df.height <= 0:\n",
    "        _die(\"EXPORT\", where, \"predictions sem target_id\", 1, [target_id])\n",
    "\n",
    "    dup = df.group_by([\"model_id\", \"resid\"]).agg(pl.len().alias(\"n\")).filter(pl.col(\"n\") > 1)\n",
    "    if dup.height > 0:\n",
    "        ex = (\n",
    "            dup.select((pl.col(\"model_id\").cast(pl.Utf8) + pl.lit(\":\") + pl.col(\"resid\").cast(pl.Utf8)).alias(\"k\"))\n",
    "            .head(8)\n",
    "            .get_column(\"k\")\n",
    "            .to_list()\n",
    "        )\n",
    "        _die(\"EXPORT\", where, \"predictions com chave duplicada no target\", int(dup.height), [f\"{target_id}:{x}\" for x in ex])\n",
    "\n",
    "    coords: dict[int, dict[int, tuple[float, float, float]]] = {}\n",
    "    for row in df.iter_rows(named=True):\n",
    "        mid = int(row[\"model_id\"])\n",
    "        if mid not in model_ids:\n",
    "            continue\n",
    "        resid = int(row[\"resid\"])\n",
    "        x = float(row[\"x\"])\n",
    "        y = float(row[\"y\"])\n",
    "        z = float(row[\"z\"])\n",
    "        if not (x == x and y == y and z == z):\n",
    "            _die(\"EXPORT\", where, \"coordenadas nao-finitas nas predictions\", 1, [f\"{target_id}:{mid}:{resid}\"])\n",
    "        per_resid = coords.setdefault(resid, {})\n",
    "        if mid in per_resid:\n",
    "            _die(\"EXPORT\", where, \"predictions com chave duplicada no target\", 1, [f\"{target_id}:{mid}:{resid}\"])\n",
    "        per_resid[mid] = (x, y, z)\n",
    "    return coords\n",
    "\n",
    "\n",
    "def _export_submission_streaming_local(sample_csv: Path, predictions_path: Path, out_csv: Path) -> None:\n",
    "    where = f\"{SCRIPT_LOC}:export_streaming_local\"\n",
    "    with sample_csv.open(\"r\", encoding=\"utf-8\", newline=\"\") as f_in, out_csv.open(\"w\", encoding=\"utf-8\", newline=\"\") as f_out:\n",
    "        reader = csv.DictReader(f_in)\n",
    "        if not reader.fieldnames:\n",
    "            _die(\"EXPORT\", where, \"sample vazio\", 1, [str(sample_csv)])\n",
    "        header = list(reader.fieldnames)\n",
    "        if \"ID\" not in header or \"resid\" not in header:\n",
    "            _die(\"EXPORT\", where, \"sample sem colunas obrigatorias ID/resid\", 1, header[:8])\n",
    "        model_ids = _model_ids_from_sample_header(header)\n",
    "\n",
    "        writer = csv.DictWriter(f_out, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "\n",
    "        current_target = None\n",
    "        current_coords = None\n",
    "        for row_idx, row in enumerate(reader, start=1):\n",
    "            tid, resid = _parse_target_resid(str(row.get(\"ID\", \"\")))\n",
    "            try:\n",
    "                resid_col = int(str(row.get(\"resid\", \"\")).strip())\n",
    "            except Exception:\n",
    "                _die(\"EXPORT\", where, \"resid invalido no sample\", 1, [f\"row={row_idx}\"])\n",
    "            if resid_col != resid:\n",
    "                _die(\"EXPORT\", where, \"sample com resid divergente do ID\", 1, [f\"{tid}_{resid}:resid={resid_col}\"])\n",
    "\n",
    "            if tid != current_target:\n",
    "                current_target = tid\n",
    "                current_coords = _load_target_coords(predictions_path, tid, model_ids)\n",
    "\n",
    "            assert current_coords is not None\n",
    "            per_resid = current_coords.get(resid)\n",
    "            if per_resid is None:\n",
    "                _die(\"EXPORT\", where, \"predictions sem resid para alvo\", 1, [f\"{tid}:{resid}\"])\n",
    "\n",
    "            for mid in model_ids:\n",
    "                xyz = per_resid.get(mid)\n",
    "                if xyz is None:\n",
    "                    _die(\"EXPORT\", where, \"predictions sem model_id para resid\", 1, [f\"{tid}:{mid}:{resid}\"])\n",
    "                x, y, z = xyz\n",
    "                row[f\"x_{mid}\"] = f\"{x}\"\n",
    "                row[f\"y_{mid}\"] = f\"{y}\"\n",
    "                row[f\"z_{mid}\"] = f\"{z}\"\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "centered_path = run_root / \"combined_predictions_centered.parquet\"\n",
    "submission_tmp_path = run_root / \"submission.csv\"\n",
    "_center_predictions_long(combined_path, centered_path)\n",
    "_export_submission_streaming_local(sample_path, centered_path, submission_tmp_path)\n",
    "_run([sys.executable, \"-m\", \"rna3d_local\", \"check-submission\", \"--sample\", str(sample_path), \"--submission\", str(submission_tmp_path)], env, repo_root)\n",
    "_assert_submission_coord_bounds(submission_tmp_path, abs_max=1000.0)\n",
    "shutil.copyfile(submission_tmp_path, submission_path)\n",
    "\n",
    "sha = hashlib.sha256(submission_path.read_bytes()).hexdigest()\n",
    "print(f\"[DONE] [{SCRIPT_LOC}] submission={submission_path}\")\n",
    "print(f\"[INFO] [{SCRIPT_LOC}] submission_sha256={sha}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
