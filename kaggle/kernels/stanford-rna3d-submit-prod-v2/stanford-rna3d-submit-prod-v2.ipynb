{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Stanford RNA3D submit notebook (TBM + DRfold2 risk router v77-alt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import hashlib\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "SCRIPT_LOC = \"submission_notebook_dynamic_tbm_drfold2_router_v77_alt_risknorm\"\n",
    "\n",
    "# Config (explicit; no silent fallback)\n",
    "USE_DRFOLD2 = True\n",
    "DRFOLD2_MAX_TARGETS = 6\n",
    "DRFOLD2_SIMILARITY_THRESHOLD = 0.62\n",
    "DRFOLD2_MAX_SEQ_LEN = 900\n",
    "DRFOLD2_N_MODELS_RUN = 1  # run DRfold2 once per target (faster)\n",
    "FINAL_N_MODELS = 5\n",
    "SUBMISSION_ABS_CLIP = 900.0\n",
    "\n",
    "\n",
    "def _die(stage: str, where: str, cause: str, impact, examples) -> None:\n",
    "    raise RuntimeError(f\"[{stage}] [{where}] {cause} | impacto={impact} | exemplos={examples}\")\n",
    "\n",
    "\n",
    "def _tail(stdout: str | None, stderr: str | None, n: int = 20) -> list[str]:\n",
    "    text = ((stdout or \"\") + \"\\n\" + (stderr or \"\")).strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    return text.splitlines()[-n:]\n",
    "\n",
    "\n",
    "def _run(cmd: list[str], env: dict, cwd: Path) -> None:\n",
    "    where = f\"{SCRIPT_LOC}:run\"\n",
    "    print(\"[RUN]\", \" \".join(cmd))\n",
    "    proc = subprocess.run(cmd, env=env, text=True, capture_output=True, cwd=str(cwd))\n",
    "    if proc.returncode != 0:\n",
    "        _die(\"PIPELINE\", where, \"comando falhou\", proc.returncode, _tail(proc.stdout, proc.stderr, 20))\n",
    "    out = (proc.stdout or \"\").strip()\n",
    "    if out:\n",
    "        print(out)\n",
    "\n",
    "\n",
    "def _supports(subcmd: str, flag: str, env: dict, cwd: Path) -> bool:\n",
    "    where = f\"{SCRIPT_LOC}:supports\"\n",
    "    proc = subprocess.run([sys.executable, \"-m\", \"rna3d_local\", subcmd, \"--help\"], env=env, text=True, capture_output=True, cwd=str(cwd))\n",
    "    if proc.returncode != 0:\n",
    "        _die(\"PIPELINE\", where, f\"falha ao consultar help de {subcmd}\", proc.returncode, _tail(proc.stdout, proc.stderr, 12))\n",
    "    txt = (proc.stdout or \"\") + \"\\n\" + (proc.stderr or \"\")\n",
    "    return flag in txt\n",
    "\n",
    "\n",
    "def _ensure_biopython(wheel_dir: Path | None):\n",
    "    where = f\"{SCRIPT_LOC}:ensure_biopython\"\n",
    "    try:\n",
    "        import Bio  # noqa: F401\n",
    "        return\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if wheel_dir is None:\n",
    "        _die(\"ENV\", where, \"Biopython ausente e diretorio de wheels nao encontrado\", 1, [])\n",
    "\n",
    "    wheels = list(wheel_dir.glob(\"biopython-*.whl\"))\n",
    "    if not wheels:\n",
    "        _die(\"ENV\", where, \"wheel biopython nao encontrado\", 1, [str(wheel_dir)])\n",
    "\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        \"-m\",\n",
    "        \"pip\",\n",
    "        \"install\",\n",
    "        \"--no-index\",\n",
    "        \"--find-links\",\n",
    "        str(wheel_dir),\n",
    "        \"biopython\",\n",
    "    ]\n",
    "    proc = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if proc.returncode != 0:\n",
    "        _die(\"ENV\", where, \"falha ao instalar biopython local\", proc.returncode, _tail(proc.stdout, proc.stderr, 12))\n",
    "\n",
    "\n",
    "def _discover_assets(input_root: Path) -> dict[str, Path | None]:\n",
    "    where = f\"{SCRIPT_LOC}:discover_assets\"\n",
    "    if not input_root.exists():\n",
    "        _die(\"LOAD\", where, \"diretorio /kaggle/input ausente\", 1, [str(input_root)])\n",
    "\n",
    "    candidates = [d for d in sorted(input_root.iterdir()) if d.is_dir()]\n",
    "    if not candidates:\n",
    "        _die(\"LOAD\", where, \"nenhum dataset montado em /kaggle/input\", 0, [])\n",
    "\n",
    "    src_root = None\n",
    "    template_index = None\n",
    "    templates = None\n",
    "    wheel_dir = None\n",
    "    drfold2_root = None\n",
    "\n",
    "    for d in candidates:\n",
    "        if src_root is None:\n",
    "            p = d / \"src\" / \"rna3d_local\" / \"cli.py\"\n",
    "            if p.exists():\n",
    "                src_root = d / \"src\"\n",
    "\n",
    "        if template_index is None:\n",
    "            p = d / \"runs\" / \"20260211_real_kaggle_baseline_full_v2\" / \"template_db\" / \"template_index.parquet\"\n",
    "            if p.exists():\n",
    "                template_index = p\n",
    "\n",
    "        if templates is None:\n",
    "            p = d / \"runs\" / \"20260211_real_kaggle_baseline_full_v2\" / \"template_db\" / \"templates.parquet\"\n",
    "            if p.exists():\n",
    "                templates = p\n",
    "\n",
    "        if wheel_dir is None:\n",
    "            p = d / \"wheels\"\n",
    "            if p.exists():\n",
    "                wheel_dir = p\n",
    "\n",
    "        if drfold2_root is None:\n",
    "            p = d / \"DRfold_infer.py\"\n",
    "            if p.exists():\n",
    "                drfold2_root = d\n",
    "\n",
    "    missing = []\n",
    "    if src_root is None:\n",
    "        missing.append(\"src/rna3d_local\")\n",
    "    if template_index is None:\n",
    "        missing.append(\"template_index.parquet\")\n",
    "    if templates is None:\n",
    "        missing.append(\"templates.parquet\")\n",
    "    if USE_DRFOLD2 and drfold2_root is None:\n",
    "        missing.append(\"DRfold_infer.py\")\n",
    "\n",
    "    if missing:\n",
    "        mounted = [d.name for d in candidates]\n",
    "        _die(\"LOAD\", where, \"ativos obrigatorios ausentes em /kaggle/input\", len(missing), missing + [f\"mounted={mounted}\"])\n",
    "\n",
    "    return {\n",
    "        \"src_root\": src_root,\n",
    "        \"template_index\": template_index,\n",
    "        \"templates\": templates,\n",
    "        \"wheel_dir\": wheel_dir,\n",
    "        \"drfold2_root\": drfold2_root,\n",
    "    }\n",
    "\n",
    "\n",
    "def _export_submission_strict_from_long(predictions_path: Path, sample_path: Path, out_path: Path):\n",
    "    where = f\"{SCRIPT_LOC}:export_strict\"\n",
    "    import pandas as pd\n",
    "\n",
    "    sample_df = pd.read_csv(sample_path)\n",
    "    pred_df = pd.read_parquet(predictions_path)\n",
    "\n",
    "    required = {\"ID\", \"model_id\", \"x\", \"y\", \"z\"}\n",
    "    miss = sorted(required - set(pred_df.columns))\n",
    "    if miss:\n",
    "        _die(\"EXPORT\", where, \"predicoes long sem colunas obrigatorias\", len(miss), miss)\n",
    "\n",
    "    pred_df[\"ID\"] = pred_df[\"ID\"].astype(str)\n",
    "    pred_df[\"model_id\"] = pd.to_numeric(pred_df[\"model_id\"], errors=\"coerce\")\n",
    "    if pred_df[\"model_id\"].isna().any():\n",
    "        bad = pred_df.loc[pred_df[\"model_id\"].isna(), \"ID\"].head(8).tolist()\n",
    "        _die(\"EXPORT\", where, \"model_id invalido em predicoes\", int(pred_df[\"model_id\"].isna().sum()), bad)\n",
    "    pred_df[\"model_id\"] = pred_df[\"model_id\"].astype(int)\n",
    "\n",
    "    dups = pred_df.duplicated([\"ID\", \"model_id\"])\n",
    "    if dups.any():\n",
    "        bad = pred_df.loc[dups, [\"ID\", \"model_id\"]].head(8).astype(str).agg(':'.join, axis=1).tolist()\n",
    "        _die(\"EXPORT\", where, \"predicoes duplicadas por ID/model_id\", int(dups.sum()), bad)\n",
    "\n",
    "    key_sample = set(sample_df[\"ID\"].astype(str).tolist())\n",
    "    key_pred = set(pred_df[\"ID\"].astype(str).tolist())\n",
    "    missing = sorted(key_sample - key_pred)\n",
    "    extra = sorted(key_pred - key_sample)\n",
    "    if missing or extra:\n",
    "        _die(\"EXPORT\", where, \"chaves de predicao nao batem com sample\", f\"missing={len(missing)} extra={len(extra)}\", missing[:4] + extra[:4])\n",
    "\n",
    "    wide = sample_df[[\"ID\", \"resname\", \"resid\"]].copy()\n",
    "    for axis in (\"x\", \"y\", \"z\"):\n",
    "        piv = pred_df.pivot(index=\"ID\", columns=\"model_id\", values=axis)\n",
    "        piv.columns = [f\"{axis}_{int(c)}\" for c in piv.columns]\n",
    "        piv = piv.reset_index()\n",
    "        wide = wide.merge(piv, on=\"ID\", how=\"left\")\n",
    "\n",
    "    expected_cols = sample_df.columns.tolist()\n",
    "    expected_pred_cols = [c for c in expected_cols if c not in (\"ID\", \"resname\", \"resid\")]\n",
    "    got_pred_cols = [c for c in wide.columns.tolist() if c not in (\"ID\", \"resname\", \"resid\")]\n",
    "    miss_pred = sorted(set(expected_pred_cols) - set(got_pred_cols))\n",
    "    extra_pred = sorted(set(got_pred_cols) - set(expected_pred_cols))\n",
    "    if miss_pred or extra_pred:\n",
    "        _die(\"EXPORT\", where, \"colunas de predicao divergentes do sample\", f\"missing={len(miss_pred)} extra={len(extra_pred)}\", miss_pred[:4] + extra_pred[:4])\n",
    "\n",
    "    wide = sample_df[[\"ID\", \"resname\", \"resid\"]].merge(wide, on=[\"ID\", \"resname\", \"resid\"], how=\"left\")\n",
    "    wide = wide.reindex(columns=expected_cols)\n",
    "\n",
    "    pred_cols = [c for c in expected_cols if c not in (\"ID\", \"resname\", \"resid\")]\n",
    "    if wide[pred_cols].isna().any().any():\n",
    "        bad = wide.loc[wide[pred_cols].isna().any(axis=1), \"ID\"].head(8).tolist()\n",
    "        _die(\"EXPORT\", where, \"submissao final contem nulos\", int(wide[pred_cols].isna().any(axis=1).sum()), bad)\n",
    "\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    wide.to_csv(out_path, index=False)\n",
    "\n",
    "\n",
    "def _prepare_drfold2_runtime(*, drfold2_input_root: Path, runtime_root: Path, env: dict) -> Path:\n",
    "    where = f\"{SCRIPT_LOC}:prepare_drfold2_runtime\"\n",
    "    infer_script = drfold2_input_root / \"DRfold_infer.py\"\n",
    "    if not infer_script.exists():\n",
    "        _die(\"DRFOLD2\", where, \"DRfold_infer.py ausente no dataset\", 1, [str(infer_script)])\n",
    "    if runtime_root.exists() and any(runtime_root.iterdir()):\n",
    "        _die(\"DRFOLD2\", where, \"runtime_root nao-vazio; use novo dir\", 1, [str(runtime_root)])\n",
    "    runtime_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # /kaggle/input is read-only; copy code/config to a writable runtime dir.\n",
    "    for child in sorted(drfold2_input_root.iterdir()):\n",
    "        dst = runtime_root / child.name\n",
    "        if child.name == \"model_hub\":\n",
    "            try:\n",
    "                dst.symlink_to(child, target_is_directory=True)\n",
    "                print(f\"[INFO] [{SCRIPT_LOC}] drfold2_model_hub_symlink dst={dst} src={child}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] [{SCRIPT_LOC}] drfold2_model_hub_symlink_failed; copying | err={e!r}\")\n",
    "                shutil.copytree(child, dst)\n",
    "            continue\n",
    "        if child.is_dir():\n",
    "            shutil.copytree(child, dst)\n",
    "        else:\n",
    "            shutil.copy2(child, dst)\n",
    "\n",
    "    arena_dir = runtime_root / \"Arena\"\n",
    "    arena_bin = arena_dir / \"Arena\"\n",
    "    if not (arena_dir / \"Makefile\").exists():\n",
    "        _die(\"DRFOLD2\", where, \"Arena/Makefile ausente\", 1, [str(arena_dir)])\n",
    "    # Always rebuild inside Kaggle to avoid binary incompatibilities.\n",
    "    proc = subprocess.run([\"make\", \"-B\"], cwd=str(arena_dir), env=env, text=True, capture_output=True)\n",
    "    if proc.returncode != 0:\n",
    "        _die(\"DRFOLD2\", where, \"falha ao compilar Arena\", proc.returncode, _tail(proc.stdout, proc.stderr, 20))\n",
    "    if not arena_bin.exists():\n",
    "        _die(\"DRFOLD2\", where, \"Arena compilado mas binario nao encontrado\", 1, [str(arena_bin)])\n",
    "\n",
    "    return runtime_root\n",
    "\n",
    "\n",
    "def _load_target_lengths(targets_csv: Path) -> dict[str, int]:\n",
    "    where = f\"{SCRIPT_LOC}:target_lengths\"\n",
    "    out: dict[str, int] = {}\n",
    "    with targets_csv.open(\"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        if reader.fieldnames is None or \"target_id\" not in reader.fieldnames or \"sequence\" not in reader.fieldnames:\n",
    "            _die(\"LOAD\", where, \"targets csv sem colunas esperadas\", 1, [str(reader.fieldnames)])\n",
    "        for row in reader:\n",
    "            tid = str(row.get(\"target_id\") or \"\").strip()\n",
    "            raw_seq = str(row.get(\"sequence\") or \"\").strip().upper()\n",
    "            seq = raw_seq.replace(\"|\", \"\")\n",
    "            if not tid:\n",
    "                _die(\"LOAD\", where, \"target_id vazio em test_sequences\", 1, [str(row)])\n",
    "            if not seq:\n",
    "                _die(\"LOAD\", where, \"sequence vazia em test_sequences\", 1, [tid])\n",
    "            out[tid] = len(seq)\n",
    "    if not out:\n",
    "        _die(\"LOAD\", where, \"test_sequences vazio\", 0, [str(targets_csv)])\n",
    "    return out\n",
    "\n",
    "\n",
    "def _select_drfold2_targets_by_risk(*, retrieval_path: Path, targets_csv: Path, out_ids_path: Path) -> list[str]:\n",
    "    where = f\"{SCRIPT_LOC}:select_drfold2_risk\"\n",
    "    import polars as pl\n",
    "\n",
    "    if DRFOLD2_MAX_TARGETS <= 0:\n",
    "        _die(\"DRFOLD2\", where, \"DRFOLD2_MAX_TARGETS invalido\", 1, [str(DRFOLD2_MAX_TARGETS)])\n",
    "    if DRFOLD2_MAX_SEQ_LEN <= 0:\n",
    "        _die(\"DRFOLD2\", where, \"DRFOLD2_MAX_SEQ_LEN invalido\", 1, [str(DRFOLD2_MAX_SEQ_LEN)])\n",
    "    if not (0.0 <= float(DRFOLD2_SIMILARITY_THRESHOLD) <= 1.0):\n",
    "        _die(\"DRFOLD2\", where, \"DRFOLD2_SIMILARITY_THRESHOLD invalido\", 1, [str(DRFOLD2_SIMILARITY_THRESHOLD)])\n",
    "\n",
    "    lf = pl.scan_parquet(str(retrieval_path))\n",
    "    names = lf.collect_schema().names()\n",
    "    if \"target_id\" not in names or \"similarity\" not in names:\n",
    "        _die(\"DRFOLD2\", where, \"retrieval sem colunas esperadas\", 1, [\"target_id\", \"similarity\"])\n",
    "\n",
    "    lengths = _load_target_lengths(targets_csv)\n",
    "    lengths_df = pl.DataFrame(\n",
    "        {\n",
    "            \"target_id\": list(lengths.keys()),\n",
    "            \"target_len\": [int(v) for v in lengths.values()],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    retrieval_df = (\n",
    "        lf.group_by(\"target_id\")\n",
    "        .agg(pl.max(\"similarity\").cast(pl.Float64).alias(\"retr_max_similarity\"))\n",
    "        .collect(streaming=True)\n",
    "    )\n",
    "    if retrieval_df.height <= 0:\n",
    "        _die(\"DRFOLD2\", where, \"retrieval vazio\", 0, [str(retrieval_path)])\n",
    "\n",
    "    ranked = (\n",
    "        retrieval_df.join(lengths_df, on=\"target_id\", how=\"inner\")\n",
    "        .filter(\n",
    "            (pl.col(\"retr_max_similarity\") < float(DRFOLD2_SIMILARITY_THRESHOLD))\n",
    "            & (pl.col(\"target_len\") <= int(DRFOLD2_MAX_SEQ_LEN))\n",
    "        )\n",
    "        .with_columns(\n",
    "            (float(DRFOLD2_SIMILARITY_THRESHOLD) - pl.col(\"retr_max_similarity\")).alias(\"_sim_gap\"),\n",
    "            (pl.col(\"target_len\").cast(pl.Float64) / float(DRFOLD2_MAX_SEQ_LEN)).alias(\"_len_ratio\"),\n",
    "        )\n",
    "        .with_columns((pl.col(\"_sim_gap\") * 10.0 + pl.col(\"_len_ratio\")).alias(\"_risk\"))\n",
    "        .sort(\n",
    "            by=[\"_risk\", \"retr_max_similarity\", \"target_len\", \"target_id\"],\n",
    "            descending=[True, False, True, False],\n",
    "        )\n",
    "        .limit(int(DRFOLD2_MAX_TARGETS))\n",
    "    )\n",
    "\n",
    "    ids = [str(x) for x in ranked.get_column(\"target_id\").to_list()]\n",
    "    out_ids_path.write_text(\"\\n\".join(ids) + (\"\\n\" if ids else \"\"), encoding=\"utf-8\")\n",
    "    if ids:\n",
    "        preview = (\n",
    "            ranked.select(\"target_id\", \"retr_max_similarity\", \"target_len\", \"_risk\")\n",
    "            .head(8)\n",
    "            .to_dicts()\n",
    "        )\n",
    "        print(f\"[INFO] [{SCRIPT_LOC}] drfold2_risk_selected n={len(ids)} ids={ids} preview={preview}\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"[INFO] [{SCRIPT_LOC}] drfold2_risk_selected n=0 \"\n",
    "            f\"threshold={DRFOLD2_SIMILARITY_THRESHOLD} max_len={DRFOLD2_MAX_SEQ_LEN}\"\n",
    "        )\n",
    "    return ids\n",
    "\n",
    "\n",
    "def _read_target_sequences_subset(targets_csv: Path, selected_ids: list[str]) -> dict[str, str]:\n",
    "    where = f\"{SCRIPT_LOC}:read_targets\"\n",
    "    selected = {str(x).strip(): None for x in selected_ids}\n",
    "    with targets_csv.open('r', encoding='utf-8', newline='') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        if reader.fieldnames is None or 'target_id' not in reader.fieldnames or 'sequence' not in reader.fieldnames:\n",
    "            _die('LOAD', where, 'targets csv sem colunas esperadas', 1, [str(reader.fieldnames)])\n",
    "        for row in reader:\n",
    "            tid = str(row.get('target_id') or '').strip()\n",
    "            if tid in selected:\n",
    "                seq = str(row.get('sequence') or '').strip().upper()\n",
    "                if not seq:\n",
    "                    _die('LOAD', where, 'sequence vazia para target', 1, [tid])\n",
    "                selected[tid] = seq\n",
    "    missing = [tid for tid, seq in selected.items() if not seq]\n",
    "    if missing:\n",
    "        _die('LOAD', where, 'target_id selecionado ausente em test_sequences', len(missing), missing[:8])\n",
    "    return {tid: str(seq) for tid, seq in selected.items() if seq}\n",
    "\n",
    "\n",
    "def _extract_c1prime_coords(pdb_path: Path, target_sequence: str):\n",
    "    where = f\"{SCRIPT_LOC}:extract_c1prime\"\n",
    "    from Bio.PDB import PDBParser\n",
    "\n",
    "    if not pdb_path.exists():\n",
    "        _die('DRFOLD2', where, 'PDB ausente', 1, [str(pdb_path)])\n",
    "\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    try:\n",
    "        structure = parser.get_structure('drfold2', str(pdb_path))\n",
    "    except Exception as exc:  # noqa: BLE001\n",
    "        _die('DRFOLD2', where, 'falha ao parsear PDB', 1, [f\"{type(exc).__name__}:{exc}\", str(pdb_path)])\n",
    "\n",
    "    models = list(structure.get_models())\n",
    "    if not models:\n",
    "        _die('DRFOLD2', where, 'PDB sem modelos', 1, [str(pdb_path)])\n",
    "\n",
    "    residues = []\n",
    "    for chain in models[0].get_chains():\n",
    "        for residue in chain.get_residues():\n",
    "            hetflag, _resseq, _icode = residue.id\n",
    "            if str(hetflag).strip():\n",
    "                continue\n",
    "            residues.append(residue)\n",
    "\n",
    "    if len(residues) != len(target_sequence):\n",
    "        _die(\n",
    "            'DRFOLD2',\n",
    "            where,\n",
    "            'PDB com numero de residuos divergente da sequencia alvo',\n",
    "            f\"expected={len(target_sequence)} got={len(residues)}\",\n",
    "            [str(pdb_path)],\n",
    "        )\n",
    "\n",
    "    coords = []\n",
    "    missing = []\n",
    "    for idx, res in enumerate(residues, start=1):\n",
    "        if not res.has_id(\"C1'\"):\n",
    "            resseq = res.id[1] if isinstance(res.id, tuple) and len(res.id) > 1 else '?'\n",
    "            missing.append(f\"idx={idx}:resseq={resseq}\")\n",
    "            continue\n",
    "        xyz = res[\"C1'\"].get_coord()\n",
    "        coords.append((float(xyz[0]), float(xyz[1]), float(xyz[2])))\n",
    "\n",
    "    if missing:\n",
    "        _die('DRFOLD2', where, \"PDB sem atomo obrigatorio C1'\", len(missing), missing[:8])\n",
    "    if len(coords) != len(target_sequence):\n",
    "        _die('DRFOLD2', where, 'coords incompletas', f\"expected={len(target_sequence)} got={len(coords)}\", [str(pdb_path)])\n",
    "\n",
    "    return coords\n",
    "\n",
    "\n",
    "def _predict_drfold2_selected(\n",
    "    *,\n",
    "    drfold2_root: Path,\n",
    "    targets_csv: Path,\n",
    "    selected_ids: list[str],\n",
    "    work_dir: Path,\n",
    "    out_parquet: Path,\n",
    "    env: dict,\n",
    ") -> tuple[list[str], list[str]]:\n",
    "    where = f\"{SCRIPT_LOC}:predict_drfold2\"\n",
    "    if not selected_ids:\n",
    "        _die('DRFOLD2', where, 'selected_ids vazio', 0, [])\n",
    "    if int(DRFOLD2_N_MODELS_RUN) not in (1, 5):\n",
    "        _die('DRFOLD2', where, 'DRFOLD2_N_MODELS_RUN invalido (suporta 1 ou 5)', 1, [str(DRFOLD2_N_MODELS_RUN)])\n",
    "    if int(FINAL_N_MODELS) != 5:\n",
    "        _die('DRFOLD2', where, 'FINAL_N_MODELS invalido (esperado 5)', 1, [str(FINAL_N_MODELS)])\n",
    "\n",
    "    infer_script = drfold2_root / 'DRfold_infer.py'\n",
    "    if not infer_script.exists():\n",
    "        _die('DRFOLD2', where, 'DRfold_infer.py ausente', 1, [str(infer_script)])\n",
    "    if not (drfold2_root / 'model_hub').exists():\n",
    "        _die('DRFOLD2', where, 'model_hub ausente', 1, [str(drfold2_root / 'model_hub')])\n",
    "\n",
    "    if work_dir.exists() and any(work_dir.iterdir()):\n",
    "        _die('DRFOLD2', where, 'work_dir nao-vazio; use novo dir', 1, [str(work_dir)])\n",
    "    work_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    seqs = _read_target_sequences_subset(targets_csv=targets_csv, selected_ids=selected_ids)\n",
    "\n",
    "    rows = []\n",
    "    succeeded_ids: list[str] = []\n",
    "    failed_errors: list[str] = []\n",
    "    for tid in selected_ids:\n",
    "        seq = seqs[tid]\n",
    "        t0 = time.time()\n",
    "        try:\n",
    "            target_dir = work_dir / tid\n",
    "            target_dir.mkdir(parents=True, exist_ok=True)\n",
    "            fasta_path = target_dir / 'target.fasta'\n",
    "            fasta_path.write_text(f\">{tid}\\n{seq}\\n\", encoding='utf-8')\n",
    "\n",
    "            cmd = [sys.executable, str(infer_script), str(fasta_path), str(target_dir)]\n",
    "            if int(DRFOLD2_N_MODELS_RUN) > 1:\n",
    "                cmd.append('1')\n",
    "\n",
    "            proc = subprocess.run(cmd, cwd=str(drfold2_root), env=env, text=True, capture_output=True)\n",
    "            if proc.returncode != 0:\n",
    "                _die('DRFOLD2', where, 'falha ao executar DRfold2', proc.returncode, _tail(proc.stdout, proc.stderr, 20))\n",
    "\n",
    "            relax_dir = target_dir / 'relax'\n",
    "            pdb_path = relax_dir / 'model_1.pdb'\n",
    "            if not pdb_path.exists():\n",
    "                # DRfold2 script may fail to produce relax output if Arena is incompatible.\n",
    "                debug = []\n",
    "                debug += _tail(proc.stdout, proc.stderr, 12)\n",
    "                debug.append(f\"ls_target_dir={sorted([p.name for p in target_dir.iterdir()])}\")\n",
    "                if (target_dir / 'folds').exists():\n",
    "                    folds = sorted([p.name for p in (target_dir / 'folds').glob('*.pdb')])\n",
    "                    debug.append(f\"folds_pdb={folds[:6]}\")\n",
    "                _die('DRFOLD2', where, 'DRfold2 nao gerou relax/model_1.pdb', 1, debug[:20])\n",
    "\n",
    "            coords = _extract_c1prime_coords(pdb_path=pdb_path, target_sequence=seq)\n",
    "\n",
    "            for model_id in range(1, 6):\n",
    "                for resid, (base, (x, y, z)) in enumerate(zip(seq, coords, strict=True), start=1):\n",
    "                    rows.append(\n",
    "                        {\n",
    "                            'branch': 'drfold2',\n",
    "                            'target_id': tid,\n",
    "                            'ID': f\"{tid}_{resid}\",\n",
    "                            'resid': resid,\n",
    "                            'resname': base,\n",
    "                            'model_id': model_id,\n",
    "                            'x': x,\n",
    "                            'y': y,\n",
    "                            'z': z,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            dt = time.time() - t0\n",
    "            succeeded_ids.append(tid)\n",
    "            print(f\"[INFO] [{SCRIPT_LOC}] drfold2_done target={tid} len={len(seq)} sec={dt:.1f}\")\n",
    "        except Exception as exc:  # noqa: BLE001\n",
    "            dt = time.time() - t0\n",
    "            msg = f\"{tid}:{type(exc).__name__}:{exc}\"\n",
    "            failed_errors.append(msg)\n",
    "            print(\n",
    "                f\"[WARN] [{SCRIPT_LOC}] [DRFOLD2] target_fail -> fallback_tbm target={tid} sec={dt:.1f} err={type(exc).__name__}:{exc}\",\n",
    "                file=sys.stderr,\n",
    "            )\n",
    "            continue\n",
    "\n",
    "    import polars as pl\n",
    "\n",
    "    if rows:\n",
    "        df = pl.DataFrame(rows)\n",
    "        df.write_parquet(str(out_parquet))\n",
    "    print(\n",
    "        f\"[INFO] [{SCRIPT_LOC}] drfold2_summary selected={len(selected_ids)} \"\n",
    "        f\"succeeded={len(succeeded_ids)} failed={len(failed_errors)}\"\n",
    "    )\n",
    "    return succeeded_ids, failed_errors\n",
    "\n",
    "\n",
    "def _build_drfold2_coord_map(drfold2_parquet: Path) -> tuple[list[str], dict[str, list[float]]]:\n",
    "    where = f\"{SCRIPT_LOC}:drfold2_map\"\n",
    "    import polars as pl\n",
    "\n",
    "    df = pl.read_parquet(str(drfold2_parquet))\n",
    "    need = {'ID', 'model_id', 'x', 'y', 'z'}\n",
    "    miss = sorted(need - set(df.columns))\n",
    "    if miss:\n",
    "        _die('DRFOLD2', where, 'drfold2 parquet sem colunas obrigatorias', len(miss), miss)\n",
    "\n",
    "    out = pl.DataFrame({'ID': df.get_column('ID').unique()})\n",
    "    for axis in ('x', 'y', 'z'):\n",
    "        piv = df.pivot(index='ID', columns='model_id', values=axis)\n",
    "        cols = [c for c in piv.columns if c != 'ID']\n",
    "        if len(cols) != 5:\n",
    "            _die('DRFOLD2', where, 'pivot incompleto (esperado 5 modelos)', len(cols), [str(cols)])\n",
    "        piv = piv.rename({c: f\"{axis}_{int(c)}\" for c in cols})\n",
    "        out = out.join(piv, on='ID', how='left')\n",
    "\n",
    "    coord_cols = []\n",
    "    for m in range(1, 6):\n",
    "        coord_cols += [f\"x_{m}\", f\"y_{m}\", f\"z_{m}\"]\n",
    "\n",
    "    mapping = {}\n",
    "    for row in out.select(['ID', *coord_cols]).iter_rows():\n",
    "        rid = str(row[0])\n",
    "        vals = [float(x) for x in row[1:]]\n",
    "        mapping[rid] = vals\n",
    "    return coord_cols, mapping\n",
    "\n",
    "\n",
    "def _patch_submission_with_drfold2(*, base_submission: Path, drfold2_parquet: Path, selected_target_ids: list[str], out_submission: Path) -> None:\n",
    "    where = f\"{SCRIPT_LOC}:patch_submission\"\n",
    "    if not selected_target_ids:\n",
    "        shutil.copyfile(base_submission, out_submission)\n",
    "        return\n",
    "\n",
    "    selected = set(selected_target_ids)\n",
    "    coord_cols, mapping = _build_drfold2_coord_map(drfold2_parquet=drfold2_parquet)\n",
    "\n",
    "    missing = []\n",
    "    touched = 0\n",
    "\n",
    "    with base_submission.open('r', encoding='utf-8', newline='') as f_in, out_submission.open('w', encoding='utf-8', newline='') as f_out:\n",
    "        reader = csv.DictReader(f_in)\n",
    "        if reader.fieldnames is None:\n",
    "            _die('EXPORT', where, 'base_submission sem header', 1, [str(base_submission)])\n",
    "        for c in coord_cols:\n",
    "            if c not in reader.fieldnames:\n",
    "                _die('EXPORT', where, 'base_submission sem coluna esperada', 1, [c])\n",
    "\n",
    "        writer = csv.DictWriter(f_out, fieldnames=reader.fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in reader:\n",
    "            rid = str(row.get('ID') or '')\n",
    "            tid = rid.split('_', 1)[0] if '_' in rid else ''\n",
    "            if tid in selected:\n",
    "                vals = mapping.get(rid)\n",
    "                if vals is None:\n",
    "                    if len(missing) < 8:\n",
    "                        missing.append(rid)\n",
    "                else:\n",
    "                    for col, val in zip(coord_cols, vals, strict=True):\n",
    "                        row[col] = repr(float(val))\n",
    "                    touched += 1\n",
    "            writer.writerow(row)\n",
    "\n",
    "    if missing:\n",
    "        _die('EXPORT', where, 'IDs selecionados sem coords drfold2', len(missing), missing)\n",
    "\n",
    "    print(f\"[INFO] [{SCRIPT_LOC}] patched_rows={touched} targets={selected_target_ids}\")\n",
    "\n",
    "\n",
    "def _submission_layout(header: list[str]) -> tuple[list[int], list[str]]:\n",
    "    where = f\"{SCRIPT_LOC}:submission_layout\"\n",
    "    mids: list[int] = []\n",
    "    for col in header:\n",
    "        if not col.startswith(\"x_\"):\n",
    "            continue\n",
    "        suffix = col.split(\"_\", 1)[1]\n",
    "        try:\n",
    "            mid = int(suffix)\n",
    "        except Exception:\n",
    "            _die(\"EXPORT\", where, \"coluna de modelo invalida\", 1, [col])\n",
    "        mids.append(mid)\n",
    "    mids = sorted(mids)\n",
    "    if not mids:\n",
    "        _die(\"EXPORT\", where, \"submission sem colunas x_<model>\", 1, header[:8])\n",
    "\n",
    "    coord_cols: list[str] = []\n",
    "    for mid in mids:\n",
    "        for axis in (\"x\", \"y\", \"z\"):\n",
    "            col = f\"{axis}_{mid}\"\n",
    "            if col not in header:\n",
    "                _die(\"EXPORT\", where, \"submission sem coluna esperada\", 1, [col])\n",
    "            coord_cols.append(col)\n",
    "    return mids, coord_cols\n",
    "\n",
    "\n",
    "def _parse_submission_id(id_value: str) -> str:\n",
    "    where = f\"{SCRIPT_LOC}:parse_submission_id\"\n",
    "    raw = str(id_value or \"\").strip()\n",
    "    if \"_\" not in raw:\n",
    "        _die(\"EXPORT\", where, \"ID invalido (esperado <target>_<resid>)\", 1, [raw])\n",
    "    target_id, resid = raw.rsplit(\"_\", 1)\n",
    "    if not target_id:\n",
    "        _die(\"EXPORT\", where, \"ID invalido (target vazio)\", 1, [raw])\n",
    "    try:\n",
    "        _ = int(resid)\n",
    "    except Exception:\n",
    "        _die(\"EXPORT\", where, \"ID invalido (resid nao inteiro)\", 1, [raw])\n",
    "    return target_id\n",
    "\n",
    "\n",
    "def _normalize_submission_coords(*, in_submission: Path, out_submission: Path, abs_clip: float) -> None:\n",
    "    where = f\"{SCRIPT_LOC}:normalize_submission\"\n",
    "    if abs_clip <= 0:\n",
    "        _die(\"EXPORT\", where, \"abs_clip invalido\", 1, [str(abs_clip)])\n",
    "\n",
    "    sums: dict[tuple[str, int, str], float] = {}\n",
    "    counts: dict[tuple[str, int, str], int] = {}\n",
    "    rows_seen = 0\n",
    "\n",
    "    with in_submission.open(\"r\", encoding=\"utf-8\", newline=\"\") as f_in:\n",
    "        reader = csv.DictReader(f_in)\n",
    "        if reader.fieldnames is None:\n",
    "            _die(\"EXPORT\", where, \"submission de entrada sem header\", 1, [str(in_submission)])\n",
    "        mids, coord_cols = _submission_layout(list(reader.fieldnames))\n",
    "        for row_idx, row in enumerate(reader, start=1):\n",
    "            rows_seen += 1\n",
    "            tid = _parse_submission_id(str(row.get(\"ID\") or \"\"))\n",
    "            for mid in mids:\n",
    "                for axis in (\"x\", \"y\", \"z\"):\n",
    "                    col = f\"{axis}_{mid}\"\n",
    "                    raw = row.get(col)\n",
    "                    try:\n",
    "                        val = float(str(raw))\n",
    "                    except Exception:\n",
    "                        _die(\"EXPORT\", where, \"valor nao-numerico na submission\", 1, [f\"row={row_idx}\", col, str(raw)])\n",
    "                    if not math.isfinite(val):\n",
    "                        _die(\"EXPORT\", where, \"valor nao-finito na submission\", 1, [f\"row={row_idx}\", col, str(raw)])\n",
    "                    key = (tid, mid, axis)\n",
    "                    sums[key] = sums.get(key, 0.0) + val\n",
    "                    counts[key] = counts.get(key, 0) + 1\n",
    "\n",
    "    if rows_seen <= 0:\n",
    "        _die(\"EXPORT\", where, \"submission de entrada vazia\", 0, [str(in_submission)])\n",
    "\n",
    "    means: dict[tuple[str, int, str], float] = {}\n",
    "    for key, total in sums.items():\n",
    "        n = counts.get(key, 0)\n",
    "        if n <= 0:\n",
    "            _die(\"EXPORT\", where, \"contador invalido ao normalizar\", 1, [str(key)])\n",
    "        means[key] = total / float(n)\n",
    "\n",
    "    clipped = 0\n",
    "    with in_submission.open(\"r\", encoding=\"utf-8\", newline=\"\") as f_in, out_submission.open(\"w\", encoding=\"utf-8\", newline=\"\") as f_out:\n",
    "        reader = csv.DictReader(f_in)\n",
    "        if reader.fieldnames is None:\n",
    "            _die(\"EXPORT\", where, \"submission de entrada sem header (2a passagem)\", 1, [str(in_submission)])\n",
    "        mids, coord_cols = _submission_layout(list(reader.fieldnames))\n",
    "        writer = csv.DictWriter(f_out, fieldnames=reader.fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row_idx, row in enumerate(reader, start=1):\n",
    "            tid = _parse_submission_id(str(row.get(\"ID\") or \"\"))\n",
    "            for mid in mids:\n",
    "                for axis in (\"x\", \"y\", \"z\"):\n",
    "                    col = f\"{axis}_{mid}\"\n",
    "                    key = (tid, mid, axis)\n",
    "                    if key not in means:\n",
    "                        _die(\"EXPORT\", where, \"mean ausente para target/model/axis\", 1, [str(key)])\n",
    "                    val = float(str(row[col])) - means[key]\n",
    "                    if val > abs_clip:\n",
    "                        val = abs_clip\n",
    "                        clipped += 1\n",
    "                    elif val < -abs_clip:\n",
    "                        val = -abs_clip\n",
    "                        clipped += 1\n",
    "                    row[col] = f\"{val:.6f}\"\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"[INFO] [{SCRIPT_LOC}] normalize_submission done rows={rows_seen} clipped={clipped} abs_clip={abs_clip:g}\")\n",
    "\n",
    "\n",
    "def _assert_submission_coord_bounds(submission_path: Path, *, abs_max: float) -> None:\n",
    "    where = f\"{SCRIPT_LOC}:submission_bounds\"\n",
    "    if abs_max <= 0:\n",
    "        _die(\"CHECK\", where, \"abs_max invalido\", 1, [str(abs_max)])\n",
    "\n",
    "    with submission_path.open(\"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        if reader.fieldnames is None:\n",
    "            _die(\"CHECK\", where, \"submission sem header\", 1, [str(submission_path)])\n",
    "        _mids, coord_cols = _submission_layout(list(reader.fieldnames))\n",
    "        bad: list[str] = []\n",
    "        for row_idx, row in enumerate(reader, start=1):\n",
    "            for col in coord_cols:\n",
    "                raw = row.get(col)\n",
    "                try:\n",
    "                    val = float(str(raw))\n",
    "                except Exception:\n",
    "                    bad.append(f\"{col}@{row_idx}:non-numeric\")\n",
    "                    if len(bad) >= 8:\n",
    "                        _die(\"CHECK\", where, \"coordenadas invalidas na submission\", len(bad), bad)\n",
    "                    continue\n",
    "                if (not math.isfinite(val)) or abs(val) > abs_max:\n",
    "                    bad.append(f\"{col}@{row_idx}:abs>{abs_max:g}\")\n",
    "                    if len(bad) >= 8:\n",
    "                        _die(\"CHECK\", where, \"coordenadas invalidas na submission\", len(bad), bad)\n",
    "        if bad:\n",
    "            _die(\"CHECK\", where, \"coordenadas invalidas na submission\", len(bad), bad)\n",
    "\n",
    "\n",
    "def _run_dynamic_pipeline(*, assets: dict[str, Path | None], sample: Path, targets: Path, run: Path, submission: Path, env: dict, repo_root: Path) -> None:\n",
    "    retrieval = run / 'retrieval_candidates.parquet'\n",
    "    tbm = run / 'tbm_predictions.parquet'\n",
    "    base_submission = run / 'submission_tbm.csv'\n",
    "\n",
    "    _run([\n",
    "        sys.executable, '-m', 'rna3d_local', 'retrieve-templates',\n",
    "        '--template-index', str(assets['template_index']),\n",
    "        '--targets', str(targets),\n",
    "        '--out', str(retrieval),\n",
    "        '--top-k', '400',\n",
    "        '--kmer-size', '3',\n",
    "        '--length-weight', '0.25',\n",
    "        '--chunk-size', '200000',\n",
    "        '--memory-budget-mb', '8192',\n",
    "        '--max-rows-in-memory', '10000000',\n",
    "    ], env, repo_root)\n",
    "\n",
    "    tbm_cmd = [\n",
    "        sys.executable, '-m', 'rna3d_local', 'predict-tbm',\n",
    "        '--retrieval', str(retrieval),\n",
    "        '--templates', str(assets['templates']),\n",
    "        '--targets', str(targets),\n",
    "        '--out', str(tbm),\n",
    "        '--n-models', str(FINAL_N_MODELS),\n",
    "        '--min-coverage', '0.01',\n",
    "        '--chunk-size', '200000',\n",
    "        '--memory-budget-mb', '8192',\n",
    "        '--max-rows-in-memory', '10000000',\n",
    "    ]\n",
    "    if _supports('predict-tbm', '--rerank-pool-size', env, repo_root):\n",
    "        tbm_cmd += ['--rerank-pool-size', '128']\n",
    "    if _supports('predict-tbm', '--mapping-mode', env, repo_root):\n",
    "        tbm_cmd += ['--mapping-mode', 'hybrid', '--projection-mode', 'template_warped']\n",
    "    _run(tbm_cmd, env, repo_root)\n",
    "\n",
    "    _export_submission_strict_from_long(predictions_path=tbm, sample_path=sample, out_path=base_submission)\n",
    "\n",
    "    final_submission = base_submission\n",
    "\n",
    "    if USE_DRFOLD2:\n",
    "        drf_input = assets.get('drfold2_root')\n",
    "        if drf_input is None:\n",
    "            _die('DRFOLD2', f\"{SCRIPT_LOC}:paths\", 'drfold2_root ausente', 1, [])\n",
    "        drf_runtime = run / 'drfold2_runtime'\n",
    "        drf_root = _prepare_drfold2_runtime(drfold2_input_root=Path(drf_input), runtime_root=drf_runtime, env=env)\n",
    "\n",
    "        ids_path = run / 'drfold2_target_ids.txt'\n",
    "        selected_ids = _select_drfold2_targets_by_risk(\n",
    "            retrieval_path=retrieval,\n",
    "            targets_csv=targets,\n",
    "            out_ids_path=ids_path,\n",
    "        )\n",
    "        if selected_ids:\n",
    "            drf_work = run / 'drfold2_work'\n",
    "            drf_pred = run / 'drfold2_predictions.parquet'\n",
    "            succeeded_ids, failed_errors = _predict_drfold2_selected(\n",
    "                drfold2_root=Path(drf_root),\n",
    "                targets_csv=targets,\n",
    "                selected_ids=selected_ids,\n",
    "                work_dir=drf_work,\n",
    "                out_parquet=drf_pred,\n",
    "                env=env,\n",
    "            )\n",
    "            failed_path = run / 'drfold2_failed_targets.txt'\n",
    "            failed_path.write_text(\"\\n\".join(failed_errors) + (\"\\n\" if failed_errors else \"\"), encoding='utf-8')\n",
    "\n",
    "            if succeeded_ids:\n",
    "                patched = run / 'submission_patched.csv'\n",
    "                _patch_submission_with_drfold2(\n",
    "                    base_submission=base_submission,\n",
    "                    drfold2_parquet=drf_pred,\n",
    "                    selected_target_ids=succeeded_ids,\n",
    "                    out_submission=patched,\n",
    "                )\n",
    "                final_submission = patched\n",
    "            else:\n",
    "                print(f\"[INFO] [{SCRIPT_LOC}] drfold2 sem alvos validos; mantendo TBM para todos os selecionados\")\n",
    "\n",
    "    normalized_submission = run / 'submission_normalized.csv'\n",
    "    _normalize_submission_coords(\n",
    "        in_submission=final_submission,\n",
    "        out_submission=normalized_submission,\n",
    "        abs_clip=float(SUBMISSION_ABS_CLIP),\n",
    "    )\n",
    "    _assert_submission_coord_bounds(normalized_submission, abs_max=float(SUBMISSION_ABS_CLIP))\n",
    "    shutil.copyfile(normalized_submission, submission)\n",
    "\n",
    "    _run([\n",
    "        sys.executable, '-m', 'rna3d_local', 'check-submission',\n",
    "        '--sample', str(sample),\n",
    "        '--submission', str(submission),\n",
    "    ], env, repo_root)\n",
    "\n",
    "\n",
    "comp = Path('/kaggle/input/stanford-rna-3d-folding-2')\n",
    "work = Path('/kaggle/working')\n",
    "input_root = Path('/kaggle/input')\n",
    "run = work / 'run_dynamic_submit_v77_alt_risknorm'\n",
    "run.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sample = comp / 'sample_submission.csv'\n",
    "targets = comp / 'test_sequences.csv'\n",
    "if not sample.exists():\n",
    "    _die('LOAD', f\"{SCRIPT_LOC}:paths\", 'sample_submission.csv ausente', 1, [str(sample)])\n",
    "if not targets.exists():\n",
    "    _die('LOAD', f\"{SCRIPT_LOC}:paths\", 'test_sequences.csv ausente', 1, [str(targets)])\n",
    "\n",
    "assets = _discover_assets(input_root)\n",
    "_ensure_biopython(assets['wheel_dir'])\n",
    "\n",
    "src_root = assets['src_root']\n",
    "if str(src_root) not in sys.path:\n",
    "    sys.path.insert(0, str(src_root))\n",
    "\n",
    "env = os.environ.copy()\n",
    "env['PYTHONPATH'] = str(src_root) + (\":\" + env['PYTHONPATH'] if env.get('PYTHONPATH') else \"\")\n",
    "env.setdefault('OMP_NUM_THREADS', '1')\n",
    "env.setdefault('MKL_NUM_THREADS', '1')\n",
    "env.setdefault('OPENBLAS_NUM_THREADS', '1')\n",
    "env.setdefault('NUMEXPR_NUM_THREADS', '1')\n",
    "\n",
    "repo_root = src_root.parent\n",
    "if not (repo_root / 'pyproject.toml').exists():\n",
    "    _die('LOAD', f\"{SCRIPT_LOC}:repo_root\", 'pyproject.toml ausente no repo_root detectado', 1, [str(repo_root)])\n",
    "\n",
    "submission = work / 'submission.csv'\n",
    "\n",
    "print(\n",
    "    f\"[INFO] [{SCRIPT_LOC}] mode=dynamic use_drfold2={USE_DRFOLD2} \"\n",
    "    f\"drfold2_max_targets={DRFOLD2_MAX_TARGETS} thr={DRFOLD2_SIMILARITY_THRESHOLD} \"\n",
    "    f\"drfold2_max_seq_len={DRFOLD2_MAX_SEQ_LEN} drfold2_n_models_run={DRFOLD2_N_MODELS_RUN} \"\n",
    "    f\"abs_clip={SUBMISSION_ABS_CLIP}\"\n",
    ")\n",
    "_run_dynamic_pipeline(\n",
    "    assets=assets,\n",
    "    sample=sample,\n",
    "    targets=targets,\n",
    "    run=run,\n",
    "    submission=submission,\n",
    "    env=env,\n",
    "    repo_root=repo_root,\n",
    ")\n",
    "print(f\"[DONE] [{SCRIPT_LOC}] dynamic_pipeline submission={submission}\")\n",
    "\n",
    "h = hashlib.sha256(submission.read_bytes()).hexdigest()\n",
    "print(f\"[INFO] [{SCRIPT_LOC}] submission_sha256={h}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
