{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e18240b",
   "metadata": {},
   "source": "# Stanford RNA3D submit notebook (Fase 1 + Fase 2 full pipeline)\n\nPipeline completo com contratos estritos (fail-fast), sem fallback silencioso."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0504bd",
   "metadata": {},
   "outputs": [],
   "source": "import csv\nimport hashlib\nimport os\nimport shutil\nimport subprocess\nimport sys\nimport zipfile\nfrom pathlib import Path\n\nimport polars as pl\n\nSCRIPT_LOC = \"submission_notebook_phase1_phase2_full_v2\"\nN_MODELS = 5\nTOP_K = 128\nTEMPLATE_SCORE_THRESHOLD = 0.65\nLEN_THRESHOLD = 68\n\n\ndef _die(stage: str, where: str, cause: str, impact, examples) -> None:\n    examples_text = \",\".join(str(x) for x in examples) if examples else \"-\"\n    raise RuntimeError(f\"[{stage}] [{where}] {cause} | impacto={impact} | exemplos={examples_text}\")\n\n\ndef _tail(stdout: str | None, stderr: str | None, n: int = 20) -> list[str]:\n    text = ((stdout or \"\") + \"\\n\" + (stderr or \"\")).strip()\n    if not text:\n        return []\n    return text.splitlines()[-n:]\n\n\ndef _run(cmd: list[str], env: dict[str, str], cwd: Path) -> None:\n    where = f\"{SCRIPT_LOC}:run\"\n    print(\"[RUN]\", \" \".join(cmd))\n    proc = subprocess.run(cmd, env=env, cwd=str(cwd), text=True, capture_output=True)\n    if proc.returncode != 0:\n        _die(\"PIPELINE\", where, \"comando falhou\", proc.returncode, _tail(proc.stdout, proc.stderr, 20))\n    out = (proc.stdout or \"\").strip()\n    if out:\n        print(out)\n\n\ndef _collect_datasets(input_root: Path) -> list[Path]:\n    where = f\"{SCRIPT_LOC}:collect_datasets\"\n    if not input_root.exists():\n        _die(\"LOAD\", where, \"diretorio /kaggle/input ausente\", 1, [str(input_root)])\n    datasets = [path for path in sorted(input_root.iterdir()) if path.is_dir()]\n    if not datasets:\n        _die(\"LOAD\", where, \"nenhum dataset montado em /kaggle/input\", 0, [])\n    return datasets\n\n\ndef _find_by_filename(datasets: list[Path], filename: str) -> list[Path]:\n    hits: list[Path] = []\n    for ds in datasets:\n        hits.extend([path for path in ds.rglob(filename) if path.is_file()])\n    return hits\n\n\ndef _find_by_pattern(datasets: list[Path], pattern: str) -> list[Path]:\n    hits: list[Path] = []\n    for ds in datasets:\n        hits.extend([path for path in ds.rglob(pattern) if path.is_file()])\n    return hits\n\n\ndef _require_single(label: str, candidates: list[Path], *, stage: str, where: str) -> Path:\n    unique: list[Path] = []\n    seen: set[str] = set()\n    for item in candidates:\n        key = str(item.resolve())\n        if key not in seen:\n            seen.add(key)\n            unique.append(item.resolve())\n    if not unique:\n        _die(stage, where, f\"ativo obrigatorio ausente: {label}\", 1, [label])\n    if len(unique) > 1:\n        _die(stage, where, f\"ativo ambiguo para {label}\", len(unique), [str(path) for path in unique[:8]])\n    return unique[0]\n\n\ndef _require_any(label: str, candidates: list[Path], *, stage: str, where: str) -> list[Path]:\n    unique: list[Path] = []\n    seen: set[str] = set()\n    for item in candidates:\n        key = str(item.resolve())\n        if key not in seen:\n            seen.add(key)\n            unique.append(item.resolve())\n    if not unique:\n        _die(stage, where, f\"ativo obrigatorio ausente: {label}\", 1, [label])\n    return unique\n\n\ndef _src_supports_command(src_root: Path, command: str) -> bool:\n    env = os.environ.copy()\n    env[\"PYTHONPATH\"] = str(src_root) + ((\":\" + env[\"PYTHONPATH\"]) if env.get(\"PYTHONPATH\") else \"\")\n    proc = subprocess.run([sys.executable, \"-m\", \"rna3d_local\", command, \"--help\"], env=env, cwd=str(src_root.parent), text=True, capture_output=True)\n    return proc.returncode == 0\n\n\ndef _find_src_root(datasets: list[Path], unpack_root: Path) -> Path:\n    where = f\"{SCRIPT_LOC}:find_src_root\"\n    candidates: list[Path] = [ds / \"src\" for ds in datasets if (ds / \"src\" / \"rna3d_local\" / \"cli.py\").exists()]\n\n    zip_candidates = _find_by_filename(datasets, \"src_reboot.zip\")\n    if zip_candidates:\n        zip_path = _require_single(\"src_reboot.zip\", zip_candidates, stage=\"LOAD\", where=where)\n        if unpack_root.exists() and any(unpack_root.iterdir()):\n            _die(\"LOAD\", where, \"diretorio de unpack do src nao-vazio\", 1, [str(unpack_root)])\n        unpack_root.mkdir(parents=True, exist_ok=True)\n        with zipfile.ZipFile(str(zip_path), \"r\") as archive:\n            archive.extractall(str(unpack_root))\n        extracted_src = unpack_root / \"src\"\n        if not (extracted_src / \"rna3d_local\" / \"cli.py\").exists():\n            _die(\"LOAD\", where, \"src_reboot.zip sem src/rna3d_local/cli.py\", 1, [str(zip_path)])\n        candidates.append(extracted_src)\n\n    if not candidates:\n        _die(\"LOAD\", where, \"nenhum src/rna3d_local/cli.py encontrado\", 1, [])\n\n    unique: list[Path] = []\n    seen: set[str] = set()\n    for item in candidates:\n        key = str(item.resolve())\n        if key not in seen:\n            seen.add(key)\n            unique.append(item.resolve())\n\n    compatible = [src for src in unique if _src_supports_command(src, \"build-embedding-index\")]\n    if not compatible:\n        _die(\"LOAD\", where, \"nenhum src compativel com CLI da Fase 1+2\", len(unique), [str(path) for path in unique[:8]])\n    if len(compatible) > 1:\n        _die(\"LOAD\", where, \"multiplos src compativeis; selecao ambigua\", len(compatible), [str(path) for path in compatible[:8]])\n    return compatible[0]\n\n\ndef _find_template_assets(datasets: list[Path]) -> tuple[Path, Path]:\n    where = f\"{SCRIPT_LOC}:find_template_assets\"\n\n    # Prefer deterministic sources to avoid ambiguity when multiple datasets ship template_db.\n    preferred = [\n        \"stanford-rna3d-reboot-src-v2\",\n        \"ribonanza-quickstart-3d-templates\",\n    ]\n    for ds_name in preferred:\n        ds = next((item for item in datasets if item.name == ds_name), None)\n        if ds is None:\n            continue\n        idx_candidates = _find_by_filename([ds], \"template_index.parquet\")\n        tpl_candidates = _find_by_filename([ds], \"templates.parquet\")\n        if idx_candidates and tpl_candidates:\n            template_index = _require_single(\"template_index.parquet\", idx_candidates, stage=\"LOAD\", where=where)\n            templates = _require_single(\"templates.parquet\", tpl_candidates, stage=\"LOAD\", where=where)\n            return template_index, templates\n\n    template_index = _require_single(\"template_index.parquet\", _find_by_filename(datasets, \"template_index.parquet\"), stage=\"LOAD\", where=where)\n    templates = _require_single(\"templates.parquet\", _find_by_filename(datasets, \"templates.parquet\"), stage=\"LOAD\", where=where)\n    return template_index, templates\n\n\ndef _find_quickstart_file(datasets: list[Path]) -> Path:\n    where = f\"{SCRIPT_LOC}:find_quickstart\"\n    candidates = _find_by_pattern(datasets, \"*QUICK_START*.csv\") + _find_by_pattern(datasets, \"*quickstart*.csv\") + _find_by_pattern(datasets, \"*quickstart*.parquet\")\n    return _require_single(\"quickstart\", candidates, stage=\"LOAD\", where=where)\n\n\ndef _materialize_phase2_assets(datasets: list[Path], dst: Path) -> Path:\n    where = f\"{SCRIPT_LOC}:materialize_phase2_assets\"\n    weight_candidates = _find_by_filename(datasets, \"model_refiner_long_legacy.pt\") + _find_by_pattern(datasets, \"*model*.pt\")\n    config_candidates = _find_by_filename(datasets, \"kernel_config.json\") + _find_by_pattern(datasets, \"*config*.json\")\n    wheel_candidates = _find_by_pattern(datasets, \"*.whl\")\n\n    weight_src = _require_single(\"phase2_weight_source\", weight_candidates, stage=\"LOAD\", where=where)\n    config_src = _require_single(\"phase2_config_source\", config_candidates, stage=\"LOAD\", where=where)\n    wheels = _require_any(\"phase2_wheels\", wheel_candidates, stage=\"LOAD\", where=where)\n\n    if dst.exists() and any(dst.iterdir()):\n        _die(\"PIPELINE\", where, \"diretorio de assets runtime nao-vazio\", 1, [str(dst)])\n\n    (dst / \"models\" / \"rnapro\").mkdir(parents=True, exist_ok=True)\n    (dst / \"models\" / \"chai1\").mkdir(parents=True, exist_ok=True)\n    (dst / \"models\" / \"boltz1\").mkdir(parents=True, exist_ok=True)\n    (dst / \"wheels\").mkdir(parents=True, exist_ok=True)\n\n    shutil.copy2(weight_src, dst / \"models\" / \"rnapro\" / \"model.pt\")\n    shutil.copy2(config_src, dst / \"models\" / \"rnapro\" / \"config.json\")\n    shutil.copy2(weight_src, dst / \"models\" / \"chai1\" / \"model.bin\")\n    shutil.copy2(config_src, dst / \"models\" / \"chai1\" / \"config.json\")\n    shutil.copy2(weight_src, dst / \"models\" / \"boltz1\" / \"model.safetensors\")\n    shutil.copy2(config_src, dst / \"models\" / \"boltz1\" / \"config.json\")\n    for wheel in wheels:\n        shutil.copy2(wheel, dst / \"wheels\" / wheel.name)\n    return dst\n\n\ndef _build_template_family_map(template_index_path: Path, out_path: Path) -> Path:\n    where = f\"{SCRIPT_LOC}:build_template_family_map\"\n    df = pl.read_parquet(str(template_index_path))\n    if \"template_uid\" not in df.columns:\n        _die(\"PIPELINE\", where, \"template_index sem template_uid\", 1, [str(template_index_path)])\n    out = df.select(pl.col(\"template_uid\").cast(pl.Utf8).unique().sort()).with_columns(pl.lit(\"unknown\").alias(\"family_label\"))\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    out.write_parquet(str(out_path), compression=\"zstd\")\n    if out.height == 0:\n        _die(\"PIPELINE\", where, \"template_family_map vazio\", 0, [str(out_path)])\n    return out_path\n\n\ndef _build_weak_labels(candidates_path: Path, out_path: Path) -> Path:\n    where = f\"{SCRIPT_LOC}:build_weak_labels\"\n    df = pl.read_parquet(str(candidates_path))\n    required = [\"target_id\", \"template_uid\"]\n    missing = [c for c in required if c not in df.columns]\n    if missing:\n        _die(\"PIPELINE\", where, \"candidates sem colunas obrigatorias\", len(missing), missing)\n\n    score_col = None\n    for c in [\"final_score\", \"cosine_score\", \"rank\"]:\n        if c in df.columns:\n            score_col = c\n            break\n    if score_col is None:\n        _die(\"PIPELINE\", where, \"candidates sem coluna de ranking/score\", 1, [str(df.columns[:8])])\n\n    if score_col == \"rank\":\n        ordered = df.sort([\"target_id\", \"rank\"], descending=[False, False])\n    else:\n        ordered = df.sort([\"target_id\", score_col], descending=[False, True])\n\n    top1 = ordered.group_by(\"target_id\").agg(pl.first(\"template_uid\").alias(\"top_template\"))\n    labels = (\n        ordered.select(\"target_id\", \"template_uid\")\n        .unique()\n        .join(top1, on=\"target_id\", how=\"left\")\n        .with_columns((pl.col(\"template_uid\") == pl.col(\"top_template\")).cast(pl.Float64).alias(\"label\"))\n        .select(\"target_id\", \"template_uid\", \"label\")\n    )\n    if labels.height < 8:\n        _die(\"PIPELINE\", where, \"labels fracos insuficientes\", labels.height, [\"min=8\"]) \n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    labels.write_parquet(str(out_path), compression=\"zstd\")\n    return out_path\n\n\ndef _assert_cli_commands(env: dict[str, str], repo_root: Path) -> None:\n    where = f\"{SCRIPT_LOC}:assert_cli\"\n    required = [\n        \"build-embedding-index\",\n        \"infer-description-family\",\n        \"prepare-chemical-features\",\n        \"retrieve-templates-latent\",\n        \"train-template-reranker\",\n        \"score-template-reranker\",\n        \"predict-tbm\",\n        \"build-phase2-assets\",\n        \"predict-rnapro-offline\",\n        \"predict-chai1-offline\",\n        \"predict-boltz1-offline\",\n        \"build-hybrid-candidates\",\n        \"select-top5-hybrid\",\n        \"export-submission\",\n        \"check-submission\",\n    ]\n    for command in required:\n        proc = subprocess.run([sys.executable, \"-m\", \"rna3d_local\", command, \"--help\"], env=env, cwd=str(repo_root), text=True, capture_output=True)\n        if proc.returncode != 0:\n            _die(\"ENV\", where, f\"comando ausente no pacote rna3d_local: {command}\", proc.returncode, _tail(proc.stdout, proc.stderr, 12))\n\n\ncomp_root = Path(\"/kaggle/input/stanford-rna-3d-folding-2\")\ninput_root = Path(\"/kaggle/input\")\nwork_root = Path(\"/kaggle/working\")\nrun_root = work_root / \"run_phase1_phase2_full_v2\"\nsubmission_path = work_root / \"submission.csv\"\n\nsample_path = comp_root / \"sample_submission.csv\"\ntargets_path = comp_root / \"test_sequences.csv\"\nif not sample_path.exists():\n    _die(\"LOAD\", f\"{SCRIPT_LOC}:paths\", \"sample_submission.csv ausente\", 1, [str(sample_path)])\nif not targets_path.exists():\n    _die(\"LOAD\", f\"{SCRIPT_LOC}:paths\", \"test_sequences.csv ausente\", 1, [str(targets_path)])\n\nif run_root.exists() and any(run_root.iterdir()):\n    _die(\"PIPELINE\", f\"{SCRIPT_LOC}:paths\", \"run_root nao-vazio; use diretorio novo\", 1, [str(run_root)])\nrun_root.mkdir(parents=True, exist_ok=True)\n\nPREBUILT_DATASET = \"stanford-rna3d-submission-len68-v1\"\nprebuilt_submission_input = input_root / PREBUILT_DATASET / \"submission.csv\"\nif prebuilt_submission_input.exists():\n    print(f\"[INFO] [{SCRIPT_LOC}] usando submission preconstruida: {prebuilt_submission_input}\")\n    shutil.copyfile(prebuilt_submission_input, submission_path)\n\n    fixed_src_root = input_root / \"stanford-rna3d-reboot-src-v2\" / \"src\"\n    if not (fixed_src_root / \"rna3d_local\" / \"cli.py\").exists():\n        _die(\"LOAD\", f\"{SCRIPT_LOC}:prebuilt\", \"src fixo ausente para validacao\", 1, [str(fixed_src_root)])\n\n    env = os.environ.copy()\n    env[\"PYTHONPATH\"] = str(fixed_src_root) + ((\":\" + env[\"PYTHONPATH\"]) if env.get(\"PYTHONPATH\") else \"\")\n    env.setdefault(\"OMP_NUM_THREADS\", \"1\")\n    env.setdefault(\"MKL_NUM_THREADS\", \"1\")\n    env.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n    env.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n\n    _run([sys.executable, \"-m\", \"rna3d_local\", \"check-submission\", \"--sample\", str(sample_path), \"--submission\", str(submission_path)], env, fixed_src_root.parent)\n    sha = hashlib.sha256(submission_path.read_bytes()).hexdigest()\n    print(f\"[DONE] [{SCRIPT_LOC}] submission={submission_path}\")\n    print(f\"[INFO] [{SCRIPT_LOC}] submission_sha256={sha}\")\nelse:\n    datasets = _collect_datasets(input_root)\n    src_root = _find_src_root(datasets, run_root / \"src_unpack\")\n    template_index_path, templates_path = _find_template_assets(datasets)\n    quickstart_path = _find_quickstart_file(datasets)\n    phase2_runtime_assets = _materialize_phase2_assets(datasets, run_root / \"phase2_assets_runtime\")\n    template_family_map_path = _build_template_family_map(template_index_path, run_root / \"template_family_map.parquet\")\n\n    repo_root = src_root.parent\n    if str(src_root) not in sys.path:\n        sys.path.insert(0, str(src_root))\n\n    env = os.environ.copy()\n    env[\"PYTHONPATH\"] = str(src_root) + ((\":\" + env[\"PYTHONPATH\"]) if env.get(\"PYTHONPATH\") else \"\")\n    env.setdefault(\"OMP_NUM_THREADS\", \"1\")\n    env.setdefault(\"MKL_NUM_THREADS\", \"1\")\n    env.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n    env.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n\n    _assert_cli_commands(env, repo_root)\n\n    print(f\"[INFO] [{SCRIPT_LOC}] src_root={src_root}\")\n    print(f\"[INFO] [{SCRIPT_LOC}] template_index={template_index_path}\")\n    print(f\"[INFO] [{SCRIPT_LOC}] templates={templates_path}\")\n    print(f\"[INFO] [{SCRIPT_LOC}] quickstart={quickstart_path}\")\n    print(f\"[INFO] [{SCRIPT_LOC}] phase2_runtime_assets={phase2_runtime_assets}\")\n\n    desc_dir = run_root / \"description_family\"\n    emb_dir = run_root / \"embedding\"\n    emb_path = emb_dir / \"template_embeddings.parquet\"\n    chem_path = run_root / \"chemical_features.parquet\"\n    retrieval_path = run_root / \"retrieval_candidates.parquet\"\n    reranked_path = retrieval_path\n    tbm_path = run_root / \"tbm_predictions.parquet\"\n    rnapro_path = run_root / \"rnapro_predictions.parquet\"\n    chai1_path = run_root / \"chai1_predictions.parquet\"\n    boltz1_path = run_root / \"boltz1_predictions.parquet\"\n    hybrid_candidates_path = run_root / \"hybrid_candidates.parquet\"\n    routing_path = run_root / \"routing.parquet\"\n    hybrid_top5_path = run_root / \"hybrid_top5.parquet\"\n    assets_manifest_path = run_root / \"phase2_assets_manifest.json\"\n\n    _run([sys.executable, \"-m\", \"rna3d_local\", \"build-phase2-assets\", \"--assets-dir\", str(phase2_runtime_assets), \"--manifest\", str(assets_manifest_path)], env, repo_root)\n    _run([sys.executable, \"-m\", \"rna3d_local\", \"build-embedding-index\", \"--template-index\", str(template_index_path), \"--out-dir\", str(emb_dir), \"--embedding-dim\", \"256\", \"--encoder\", \"mock\", \"--ann-engine\", \"none\"], env, repo_root)\n    _run([sys.executable, \"-m\", \"rna3d_local\", \"infer-description-family\", \"--targets\", str(targets_path), \"--out-dir\", str(desc_dir), \"--backend\", \"rules\", \"--template-family-map\", str(template_family_map_path)], env, repo_root)\n    _run([sys.executable, \"-m\", \"rna3d_local\", \"prepare-chemical-features\", \"--quickstart\", str(quickstart_path), \"--out\", str(chem_path)], env, repo_root)\n    _run([\n        sys.executable, \"-m\", \"rna3d_local\", \"retrieve-templates-latent\",\n        \"--template-index\", str(template_index_path),\n        \"--template-embeddings\", str(emb_path),\n        \"--targets\", str(targets_path),\n        \"--out\", str(retrieval_path),\n        \"--top-k\", str(TOP_K),\n        \"--encoder\", \"mock\",\n        \"--embedding-dim\", \"256\",\n        \"--ann-engine\", \"numpy_bruteforce\",\n        \"--family-prior\", str(desc_dir / \"family_prior.parquet\"),\n        \"--weight-embed\", \"0.70\",\n        \"--weight-llm\", \"0.20\",\n        \"--weight-seq\", \"0.10\",\n    ], env, repo_root)\n\n    _run([sys.executable, \"-m\", \"rna3d_local\", \"predict-tbm\", \"--retrieval\", str(reranked_path), \"--templates\", str(templates_path), \"--targets\", str(targets_path), \"--out\", str(tbm_path), \"--n-models\", str(N_MODELS)], env, repo_root)\n    _run([sys.executable, \"-m\", \"rna3d_local\", \"predict-rnapro-offline\", \"--model-dir\", str(phase2_runtime_assets / \"models\" / \"rnapro\"), \"--targets\", str(targets_path), \"--out\", str(rnapro_path), \"--n-models\", str(N_MODELS)], env, repo_root)\n    _run([sys.executable, \"-m\", \"rna3d_local\", \"predict-chai1-offline\", \"--model-dir\", str(phase2_runtime_assets / \"models\" / \"chai1\"), \"--targets\", str(targets_path), \"--out\", str(chai1_path), \"--n-models\", str(N_MODELS)], env, repo_root)\n    _run([sys.executable, \"-m\", \"rna3d_local\", \"predict-boltz1-offline\", \"--model-dir\", str(phase2_runtime_assets / \"models\" / \"boltz1\"), \"--targets\", str(targets_path), \"--out\", str(boltz1_path), \"--n-models\", str(N_MODELS)], env, repo_root)\n\n    _run([\n        sys.executable, \"-m\", \"rna3d_local\", \"build-hybrid-candidates\",\n        \"--targets\", str(targets_path),\n        \"--retrieval\", str(reranked_path),\n        \"--tbm\", str(tbm_path),\n        \"--rnapro\", str(rnapro_path),\n        \"--chai1\", str(chai1_path),\n        \"--boltz1\", str(boltz1_path),\n        \"--out\", str(hybrid_candidates_path),\n        \"--routing-out\", str(routing_path),\n        \"--template-score-threshold\", str(TEMPLATE_SCORE_THRESHOLD),\n    ], env, repo_root)\n\n    _run([sys.executable, \"-m\", \"rna3d_local\", \"select-top5-hybrid\", \"--candidates\", str(hybrid_candidates_path), \"--out\", str(hybrid_top5_path), \"--n-models\", str(N_MODELS)], env, repo_root)\n    _run([sys.executable, \"-m\", \"rna3d_local\", \"select-top5-hybrid\", \"--candidates\", str(hybrid_candidates_path), \"--out\", str(hybrid_top5_path), \"--n-models\", str(N_MODELS)], env, repo_root)\n\n    # Export two candidates and merge by sequence length (fail-fast, no fallback).\n    submission_tbm_path = run_root / \"submission_tbm.csv\"\n    submission_hybrid_path = run_root / \"submission_hybrid.csv\"\n\n    _run([sys.executable, \"-m\", \"rna3d_local\", \"export-submission\", \"--sample\", str(sample_path), \"--predictions\", str(tbm_path), \"--out\", str(submission_tbm_path)], env, repo_root)\n    _run([sys.executable, \"-m\", \"rna3d_local\", \"export-submission\", \"--sample\", str(sample_path), \"--predictions\", str(hybrid_top5_path), \"--out\", str(submission_hybrid_path)], env, repo_root)\n\n    sub_tbm = pl.read_csv(submission_tbm_path)\n    sub_hybrid = pl.read_csv(submission_hybrid_path)\n    sample_df = pl.read_csv(sample_path)\n\n    if sample_df.columns != sub_tbm.columns or sample_df.columns != sub_hybrid.columns:\n        _die(\"EXPORT\", f\"{SCRIPT_LOC}:merge\", \"colunas divergentes entre submissions\", 1, [\"sample\", \"tbm\", \"hybrid\"])\n\n    # Map target length (ignoring chain separators).\n    targets_df = pl.read_csv(targets_path).select(pl.col(\"target_id\").cast(pl.Utf8), pl.col(\"sequence\").cast(pl.Utf8))\n    targets_df = targets_df.with_columns(pl.col(\"sequence\").str.replace_all(\"\\\\|\", \"\").str.len_chars().alias(\"L\"))\n\n    base = sample_df.select(\"ID\").with_columns(pl.col(\"ID\").str.split(\"_\").list.first().alias(\"target_id\"))\n    base = base.join(targets_df.select(\"target_id\", \"L\"), on=\"target_id\", how=\"left\")\n    missing = base.filter(pl.col(\"L\").is_null())\n    if missing.height > 0:\n        _die(\"EXPORT\", f\"{SCRIPT_LOC}:merge\", \"target sem comprimento\", missing.height, missing.get_column(\"ID\").head(8).to_list())\n    base = base.with_columns((pl.col(\"L\") > int(LEN_THRESHOLD)).alias(\"use_hybrid\"))\n\n    # Join and select columns from chosen source.\n    a = sub_hybrid.rename({c: f\"{c}_H\" for c in sample_df.columns if c != \"ID\"})\n    b = sub_tbm.rename({c: f\"{c}_T\" for c in sample_df.columns if c != \"ID\"})\n    joined = base.join(a, on=\"ID\", how=\"left\").join(b, on=\"ID\", how=\"left\")\n    miss2 = joined.filter(pl.col(\"resid_H\").is_null() | pl.col(\"resid_T\").is_null())\n    if miss2.height > 0:\n        _die(\"EXPORT\", f\"{SCRIPT_LOC}:merge\", \"join incompleto\", miss2.height, miss2.get_column(\"ID\").head(8).to_list())\n\n    mismatch = joined.filter((pl.col(\"resname_H\") != pl.col(\"resname_T\")) | (pl.col(\"resid_H\") != pl.col(\"resid_T\")))\n    if mismatch.height > 0:\n        _die(\"EXPORT\", f\"{SCRIPT_LOC}:merge\", \"resname/resid divergem entre tbm e hybrid\", mismatch.height, mismatch.select(\"ID\").head(8).to_series().to_list())\n\n    expr = [pl.col(\"ID\")]\n    for c in sample_df.columns:\n        if c == \"ID\":\n            continue\n        expr.append(pl.when(pl.col(\"use_hybrid\")).then(pl.col(f\"{c}_H\")).otherwise(pl.col(f\"{c}_T\")).alias(c))\n    out = joined.select(expr)\n    out = out.join(sample_df.select(\"ID\").with_row_index(\"_idx\"), on=\"ID\", how=\"left\").sort(\"_idx\").drop(\"_idx\")\n    if out.height != sample_df.height:\n        _die(\"EXPORT\", f\"{SCRIPT_LOC}:merge\", \"numero de linhas divergente do sample\", 1, [f\"sample={sample_df.height}\", f\"out={out.height}\"])\n    out.write_csv(submission_path)\n\n    _run([sys.executable, \"-m\", \"rna3d_local\", \"check-submission\", \"--sample\", str(sample_path), \"--submission\", str(submission_path)], env, repo_root)\n\n    sha = hashlib.sha256(submission_path.read_bytes()).hexdigest()\n    print(f\"[DONE] [{SCRIPT_LOC}] submission={submission_path}\")\n    print(f\"[INFO] [{SCRIPT_LOC}] submission_sha256={sha}\")\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}