{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e18240b",
   "metadata": {},
   "source": "# Stanford RNA3D submit notebook (Fase 1 + Fase 2 full pipeline)\n\nPipeline completo com contratos estritos (fail-fast), sem fallback silencioso."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0504bd",
   "metadata": {},
   "outputs": [],
   "source": "import csv\nimport hashlib\nimport os\nimport shutil\nimport subprocess\nimport sys\nimport zipfile\nfrom pathlib import Path\n\nimport polars as pl\n\nSCRIPT_LOC = \"submission_notebook_phase1_phase2_full_v2\"\nN_MODELS = 5\nTOP_K = 128\nTEMPLATE_SCORE_THRESHOLD = 0.65\nLEN_THRESHOLD = 68\n\n\ndef _die(stage: str, where: str, cause: str, impact, examples) -> None:\n    examples_text = \",\".join(str(x) for x in examples) if examples else \"-\"\n    raise RuntimeError(f\"[{stage}] [{where}] {cause} | impacto={impact} | exemplos={examples_text}\")\n\n\ndef _tail(stdout: str | None, stderr: str | None, n: int = 20) -> list[str]:\n    text = ((stdout or \"\") + \"\\n\" + (stderr or \"\")).strip()\n    if not text:\n        return []\n    return text.splitlines()[-n:]\n\n\ndef _run(cmd: list[str], env: dict[str, str], cwd: Path) -> None:\n    where = f\"{SCRIPT_LOC}:run\"\n    print(\"[RUN]\", \" \".join(cmd))\n    proc = subprocess.run(cmd, env=env, cwd=str(cwd), text=True, capture_output=True)\n    if proc.returncode != 0:\n        _die(\"PIPELINE\", where, \"comando falhou\", proc.returncode, _tail(proc.stdout, proc.stderr, 20))\n    out = (proc.stdout or \"\").strip()\n    if out:\n        print(out)\n\n\ndef _collect_datasets(input_root: Path) -> list[Path]:\n    where = f\"{SCRIPT_LOC}:collect_datasets\"\n    if not input_root.exists():\n        _die(\"LOAD\", where, \"diretorio /kaggle/input ausente\", 1, [str(input_root)])\n    datasets = [path for path in sorted(input_root.iterdir()) if path.is_dir()]\n    if not datasets:\n        _die(\"LOAD\", where, \"nenhum dataset montado em /kaggle/input\", 0, [])\n    return datasets\n\n\ndef _find_by_filename(datasets: list[Path], filename: str) -> list[Path]:\n    hits: list[Path] = []\n    for ds in datasets:\n        hits.extend([path for path in ds.rglob(filename) if path.is_file()])\n    return hits\n\n\ndef _find_by_pattern(datasets: list[Path], pattern: str) -> list[Path]:\n    hits: list[Path] = []\n    for ds in datasets:\n        hits.extend([path for path in ds.rglob(pattern) if path.is_file()])\n    return hits\n\n\ndef _require_single(label: str, candidates: list[Path], *, stage: str, where: str) -> Path:\n    unique: list[Path] = []\n    seen: set[str] = set()\n    for item in candidates:\n        key = str(item.resolve())\n        if key not in seen:\n            seen.add(key)\n            unique.append(item.resolve())\n    if not unique:\n        _die(stage, where, f\"ativo obrigatorio ausente: {label}\", 1, [label])\n    if len(unique) > 1:\n        _die(stage, where, f\"ativo ambiguo para {label}\", len(unique), [str(path) for path in unique[:8]])\n    return unique[0]\n\n\ndef _require_any(label: str, candidates: list[Path], *, stage: str, where: str) -> list[Path]:\n    unique: list[Path] = []\n    seen: set[str] = set()\n    for item in candidates:\n        key = str(item.resolve())\n        if key not in seen:\n            seen.add(key)\n            unique.append(item.resolve())\n    if not unique:\n        _die(stage, where, f\"ativo obrigatorio ausente: {label}\", 1, [label])\n    return unique\n\n\ndef _src_supports_command(src_root: Path, command: str) -> bool:\n    env = os.environ.copy()\n    env[\"PYTHONPATH\"] = str(src_root) + ((\":\" + env[\"PYTHONPATH\"]) if env.get(\"PYTHONPATH\") else \"\")\n    proc = subprocess.run([sys.executable, \"-m\", \"rna3d_local\", command, \"--help\"], env=env, cwd=str(src_root.parent), text=True, capture_output=True)\n    return proc.returncode == 0\n\n\ndef _find_src_root(datasets: list[Path], unpack_root: Path) -> Path:\n    where = f\"{SCRIPT_LOC}:find_src_root\"\n    candidates: list[Path] = [ds / \"src\" for ds in datasets if (ds / \"src\" / \"rna3d_local\" / \"cli.py\").exists()]\n\n    zip_candidates = _find_by_filename(datasets, \"src_reboot.zip\")\n    if zip_candidates:\n        zip_path = _require_single(\"src_reboot.zip\", zip_candidates, stage=\"LOAD\", where=where)\n        if unpack_root.exists() and any(unpack_root.iterdir()):\n            _die(\"LOAD\", where, \"diretorio de unpack do src nao-vazio\", 1, [str(unpack_root)])\n        unpack_root.mkdir(parents=True, exist_ok=True)\n        with zipfile.ZipFile(str(zip_path), \"r\") as archive:\n            archive.extractall(str(unpack_root))\n        extracted_src = unpack_root / \"src\"\n        if not (extracted_src / \"rna3d_local\" / \"cli.py\").exists():\n            _die(\"LOAD\", where, \"src_reboot.zip sem src/rna3d_local/cli.py\", 1, [str(zip_path)])\n        candidates.append(extracted_src)\n\n    if not candidates:\n        _die(\"LOAD\", where, \"nenhum src/rna3d_local/cli.py encontrado\", 1, [])\n\n    unique: list[Path] = []\n    seen: set[str] = set()\n    for item in candidates:\n        key = str(item.resolve())\n        if key not in seen:\n            seen.add(key)\n            unique.append(item.resolve())\n\n    compatible = [src for src in unique if _src_supports_command(src, \"build-embedding-index\")]\n    if not compatible:\n        _die(\"LOAD\", where, \"nenhum src compativel com CLI da Fase 1+2\", len(unique), [str(path) for path in unique[:8]])\n    if len(compatible) > 1:\n        _die(\"LOAD\", where, \"multiplos src compativeis; selecao ambigua\", len(compatible), [str(path) for path in compatible[:8]])\n    return compatible[0]\n\n\ndef _find_template_assets(datasets: list[Path]) -> tuple[Path, Path]:\n    where = f\"{SCRIPT_LOC}:find_template_assets\"\n\n    # Prefer deterministic sources to avoid ambiguity when multiple datasets ship template_db.\n    preferred = [\n        \"stanford-rna3d-reboot-src-v2\",\n        \"ribonanza-quickstart-3d-templates\",\n    ]\n    for ds_name in preferred:\n        ds = next((item for item in datasets if item.name == ds_name), None)\n        if ds is None:\n            continue\n        idx_candidates = _find_by_filename([ds], \"template_index.parquet\")\n        tpl_candidates = _find_by_filename([ds], \"templates.parquet\")\n        if idx_candidates and tpl_candidates:\n            template_index = _require_single(\"template_index.parquet\", idx_candidates, stage=\"LOAD\", where=where)\n            templates = _require_single(\"templates.parquet\", tpl_candidates, stage=\"LOAD\", where=where)\n            return template_index, templates\n\n    template_index = _require_single(\"template_index.parquet\", _find_by_filename(datasets, \"template_index.parquet\"), stage=\"LOAD\", where=where)\n    templates = _require_single(\"templates.parquet\", _find_by_filename(datasets, \"templates.parquet\"), stage=\"LOAD\", where=where)\n    return template_index, templates\n\n\ndef _find_quickstart_file(datasets: list[Path]) -> Path:\n    where = f\"{SCRIPT_LOC}:find_quickstart\"\n    candidates = _find_by_pattern(datasets, \"*QUICK_START*.csv\") + _find_by_pattern(datasets, \"*quickstart*.csv\") + _find_by_pattern(datasets, \"*quickstart*.parquet\")\n    return _require_single(\"quickstart\", candidates, stage=\"LOAD\", where=where)\n\n\ndef _materialize_phase2_assets(datasets: list[Path], dst: Path) -> Path:\n    where = f\"{SCRIPT_LOC}:materialize_phase2_assets\"\n    # Locate a phase2 assets root that matches the rna3d_local offline runners contract.\n    # Expected layout: <root>/models/{rnapro,chai1,boltz1}/...\n    required_markers = [\n        (\"rnapro\", Path(\"models/rnapro/rnapro-public-best-500m.ckpt\")),\n        (\"chai1\", Path(\"models/chai1/models_v2/trunk.pt\")),\n        (\"boltz1\", Path(\"models/boltz1/boltz1_conf.ckpt\")),\n    ]\n    roots: list[Path] = []\n    for ds in datasets:\n        for cand in [\n            ds,\n            ds / \"export\" / \"kaggle_assets\",\n            ds / \"kaggle_assets\",\n            ds / \"export\" / \"kaggle_assets\" / \"export\" / \"kaggle_assets\",\n        ]:\n            ok = True\n            for _label, rel in required_markers:\n                if not (cand / rel).exists():\n                    ok = False\n                    break\n            if ok:\n                roots.append(cand.resolve())\n    uniq: list[Path] = []\n    seen: set[str] = set()\n    for r in roots:\n        k = str(r)\n        if k not in seen:\n            seen.add(k)\n            uniq.append(r)\n    if not uniq:\n        _die(\"LOAD\", where, \"assets phase2 nao encontrados (markers ausentes)\", 1, [str(m[1]) for m in required_markers])\n    if len(uniq) > 1:\n        _die(\"LOAD\", where, \"assets phase2 ambiguos (multiplos roots)\", len(uniq), [str(p) for p in uniq[:8]])\n    return uniq[0]\n\ndef _build_template_family_map(template_index_path: Path, out_path: Path) -> Path:\n    where = f\"{SCRIPT_LOC}:build_template_family_map\"\n    df = pl.read_parquet(str(template_index_path))\n    if \"template_uid\" not in df.columns:\n        _die(\"PIPELINE\", where, \"template_index sem template_uid\", 1, [str(template_index_path)])\n    out = df.select(pl.col(\"template_uid\").cast(pl.Utf8).unique().sort()).with_columns(pl.lit(\"unknown\").alias(\"family_label\"))\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    out.write_parquet(str(out_path), compression=\"zstd\")\n    if out.height == 0:\n        _die(\"PIPELINE\", where, \"template_family_map vazio\", 0, [str(out_path)])\n    return out_path\n\n\ndef _build_weak_labels(candidates_path: Path, out_path: Path) -> Path:\n    where = f\"{SCRIPT_LOC}:build_weak_labels\"\n    df = pl.read_parquet(str(candidates_path))\n    required = [\"target_id\", \"template_uid\"]\n    missing = [c for c in required if c not in df.columns]\n    if missing:\n        _die(\"PIPELINE\", where, \"candidates sem colunas obrigatorias\", len(missing), missing)\n\n    score_col = None\n    for c in [\"final_score\", \"cosine_score\", \"rank\"]:\n        if c in df.columns:\n            score_col = c\n            break\n    if score_col is None:\n        _die(\"PIPELINE\", where, \"candidates sem coluna de ranking/score\", 1, [str(df.columns[:8])])\n\n    if score_col == \"rank\":\n        ordered = df.sort([\"target_id\", \"rank\"], descending=[False, False])\n    else:\n        ordered = df.sort([\"target_id\", score_col], descending=[False, True])\n\n    top1 = ordered.group_by(\"target_id\").agg(pl.first(\"template_uid\").alias(\"top_template\"))\n    labels = (\n        ordered.select(\"target_id\", \"template_uid\")\n        .unique()\n        .join(top1, on=\"target_id\", how=\"left\")\n        .with_columns((pl.col(\"template_uid\") == pl.col(\"top_template\")).cast(pl.Float64).alias(\"label\"))\n        .select(\"target_id\", \"template_uid\", \"label\")\n    )\n    if labels.height < 8:\n        _die(\"PIPELINE\", where, \"labels fracos insuficientes\", labels.height, [\"min=8\"]) \n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    labels.write_parquet(str(out_path), compression=\"zstd\")\n    return out_path\n\n\ndef _assert_cli_commands(env: dict[str, str], repo_root: Path) -> None:\n    where = f\"{SCRIPT_LOC}:assert_cli\"\n    required = [\n        \"build-embedding-index\",\n        \"infer-description-family\",\n        \"retrieve-templates-latent\",\n        \"predict-tbm\",\n        \"export-submission\",\n        \"check-submission\",\n    ]\n    for command in required:\n        proc = subprocess.run([sys.executable, \"-m\", \"rna3d_local\", command, \"--help\"], env=env, cwd=str(repo_root), text=True, capture_output=True)\n        if proc.returncode != 0:\n            _die(\"ENV\", where, f\"comando ausente no pacote rna3d_local: {command}\", proc.returncode, _tail(proc.stdout, proc.stderr, 12))\n\n\nimport uuid\n\ncomp_root = Path(\"/kaggle/input/stanford-rna-3d-folding-2\")\ninput_root = Path(\"/kaggle/input\")\nwork_root = Path(\"/kaggle/working\")\nrun_root = work_root / f\"run_{SCRIPT_LOC}_{uuid.uuid4().hex[:8]}\"\nsubmission_path = work_root / \"submission.csv\"\n\nsample_path = comp_root / \"sample_submission.csv\"\ntargets_path = comp_root / \"test_sequences.csv\"\nif not sample_path.exists():\n    _die(\"LOAD\", f\"{SCRIPT_LOC}:paths\", \"sample_submission.csv ausente\", 1, [str(sample_path)])\nif not targets_path.exists():\n    _die(\"LOAD\", f\"{SCRIPT_LOC}:paths\", \"test_sequences.csv ausente\", 1, [str(targets_path)])\n\nrun_root.mkdir(parents=True, exist_ok=False)\n\ndatasets = _collect_datasets(input_root)\nsrc_root = _find_src_root(datasets, run_root / \"src_unpack\")\ntemplate_index_path, templates_path = _find_template_assets(datasets)\ntemplate_family_map_path = _build_template_family_map(template_index_path, run_root / \"template_family_map.parquet\")\n\nrepo_root = src_root.parent\nif str(src_root) not in sys.path:\n    sys.path.insert(0, str(src_root))\n\nenv = os.environ.copy()\nenv[\"PYTHONPATH\"] = str(src_root) + ((\":\" + env[\"PYTHONPATH\"]) if env.get(\"PYTHONPATH\") else \"\")\nenv.setdefault(\"OMP_NUM_THREADS\", \"1\")\nenv.setdefault(\"MKL_NUM_THREADS\", \"1\")\nenv.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\nenv.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n\n_assert_cli_commands(env, repo_root)\n\nprint(f\"[INFO] [{SCRIPT_LOC}] run_root={run_root}\")\nprint(f\"[INFO] [{SCRIPT_LOC}] src_root={src_root}\")\nprint(f\"[INFO] [{SCRIPT_LOC}] template_index={template_index_path}\")\nprint(f\"[INFO] [{SCRIPT_LOC}] templates={templates_path}\")\n\ndesc_dir = run_root / \"description_family\"\nemb_dir = run_root / \"embedding\"\nemb_path = emb_dir / \"template_embeddings.parquet\"\nretrieval_path = run_root / \"retrieval_candidates.parquet\"\nretrieval_tbm_path = run_root / \"retrieval_candidates_tbm.parquet\"\ntargets_tbm_path = run_root / \"targets_tbm.csv\"\ntargets_fallback_path = run_root / \"targets_fallback.csv\"\ntbm_path = run_root / \"tbm_predictions.parquet\"\ndrfold_path = run_root / \"drfold_predictions.parquet\"\ncombined_path = run_root / \"combined_predictions.parquet\"\n\n_run([sys.executable, \"-m\", \"rna3d_local\", \"build-embedding-index\", \"--template-index\", str(template_index_path), \"--out-dir\", str(emb_dir), \"--embedding-dim\", \"256\", \"--encoder\", \"mock\", \"--ann-engine\", \"none\"], env, repo_root)\n_run([sys.executable, \"-m\", \"rna3d_local\", \"infer-description-family\", \"--targets\", str(targets_path), \"--out-dir\", str(desc_dir), \"--backend\", \"rules\", \"--template-family-map\", str(template_family_map_path)], env, repo_root)\n_run([\n    sys.executable, \"-m\", \"rna3d_local\", \"retrieve-templates-latent\",\n    \"--template-index\", str(template_index_path),\n    \"--template-embeddings\", str(emb_path),\n    \"--targets\", str(targets_path),\n    \"--out\", str(retrieval_path),\n    \"--top-k\", str(TOP_K),\n    \"--encoder\", \"mock\",\n    \"--embedding-dim\", \"256\",\n    \"--ann-engine\", \"numpy_bruteforce\",\n    \"--family-prior\", str(desc_dir / \"family_prior.parquet\"),\n    \"--weight-embed\", \"0.70\",\n    \"--weight-llm\", \"0.20\",\n    \"--weight-seq\", \"0.10\",\n], env, repo_root)\n\n# Compute which targets can be covered by at least one contiguous template with length >= target_len.\ntargets_df = pl.read_csv(targets_path)\nif \"target_id\" not in targets_df.columns or \"sequence\" not in targets_df.columns:\n    _die(\"LOAD\", f\"{SCRIPT_LOC}:targets\", \"targets sem colunas target_id/sequence\", 1, [str(targets_df.columns)])\ntargets_df = targets_df.with_columns(pl.col(\"target_id\").cast(pl.Utf8), pl.col(\"sequence\").cast(pl.Utf8))\ntargets_len = targets_df.select(pl.col(\"target_id\"), pl.col(\"sequence\").str.replace_all(r\"\\|\", \"\").str.len_chars().alias(\"target_len\"))\n\ntpl_stats = pl.scan_parquet(str(templates_path)).select(pl.col(\"template_uid\").cast(pl.Utf8), pl.col(\"resid\").cast(pl.Int32))\ntpl_stats = tpl_stats.group_by(\"template_uid\").agg(pl.col(\"resid\").min().alias(\"min_resid\"), pl.col(\"resid\").max().alias(\"max_resid\"), pl.col(\"resid\").n_unique().alias(\"n_unique\"))\ntpl_stats = tpl_stats.with_columns((pl.col(\"max_resid\") - pl.col(\"min_resid\") + 1).cast(pl.Int32).alias(\"tpl_len\"))\ntpl_stats = tpl_stats.with_columns((pl.col(\"n_unique\") == pl.col(\"tpl_len\")).alias(\"contiguous\"))\n\nretr_lf = pl.scan_parquet(str(retrieval_path)).select(pl.col(\"target_id\").cast(pl.Utf8), pl.col(\"template_uid\").cast(pl.Utf8)).unique()\nsupported = retr_lf.join(targets_len.lazy(), on=\"target_id\", how=\"inner\").join(tpl_stats, on=\"template_uid\", how=\"inner\")\nsupported = supported.filter(pl.col(\"contiguous\") & (pl.col(\"tpl_len\") >= pl.col(\"target_len\"))).select(pl.col(\"target_id\")).unique().collect()\nsupported_ids = set(supported.get_column(\"target_id\").to_list())\nall_ids = set(targets_len.get_column(\"target_id\").to_list())\nfallback_ids = sorted(all_ids - supported_ids)\nprint(f\"[INFO] [{SCRIPT_LOC}] tbm_supported={len(supported_ids)} fallback={len(fallback_ids)}\")\nif len(supported_ids) == 0:\n    _die(\"PIPELINE\", f\"{SCRIPT_LOC}:coverage\", \"nenhum alvo com template valido para TBM\", 0, [])\n\n# Write filtered targets/retrieval for TBM.\ntargets_df.filter(pl.col(\"target_id\").is_in(sorted(supported_ids))).write_csv(targets_tbm_path)\ntargets_df.filter(pl.col(\"target_id\").is_in(fallback_ids)).write_csv(targets_fallback_path)\npl.read_parquet(retrieval_path).filter(pl.col(\"target_id\").is_in(sorted(supported_ids))).write_parquet(retrieval_tbm_path)\n\n_run([sys.executable, \"-m\", \"rna3d_local\", \"predict-tbm\", \"--retrieval\", str(retrieval_tbm_path), \"--templates\", str(templates_path), \"--targets\", str(targets_tbm_path), \"--out\", str(tbm_path), \"--n-models\", str(N_MODELS)], env, repo_root)\n\ndef _normalize_seq(seq: str) -> str:\n    raw = str(seq or \"\").strip().upper().replace(\"T\", \"U\")\n    return \"\".join(ch for ch in raw if ch not in {\"|\", \" \", \"\\t\", \"\\n\", \"\\r\"})\n\ndef _parse_c1prime_coords(pdb_path: Path, *, target_id: str, seq: str):\n    coords = {}\n    try:\n        for line in pdb_path.read_text(\"utf-8\", errors=\"replace\").splitlines():\n            if not line.startswith(\"ATOM\"):\n                continue\n            atom = line[12:16].strip()\n            if atom != \"C1'\":\n                continue\n            try:\n                resid = int(line[22:26].strip())\n                x = float(line[30:38].strip())\n                y = float(line[38:46].strip())\n                z = float(line[46:54].strip())\n            except Exception:\n                continue\n            coords[resid] = (x, y, z)\n    except Exception as exc:\n        _die(\"DRFOLD2\", f\"{SCRIPT_LOC}:parse_pdb\", \"falha ao ler pdb\", 1, [target_id, str(pdb_path), f\"{type(exc).__name__}:{exc}\"])\n    out = []\n    if len(seq) == 0:\n        _die(\"DRFOLD2\", f\"{SCRIPT_LOC}:parse_pdb\", \"sequencia vazia\", 1, [target_id])\n    for i,ch in enumerate(seq, start=1):\n        if i not in coords:\n            _die(\"DRFOLD2\", f\"{SCRIPT_LOC}:parse_pdb\", \"C1' ausente\", 1, [f\"{target_id}:{i}\", str(pdb_path)])\n        x, y, z = coords[i]\n        out.append((i, ch, float(x), float(y), float(z)))\n    return out\n\n# DRfold2 fallback for targets without valid TBM templates.\ndrfold_parts = []\nif fallback_ids:\n    drfold_candidates = _find_by_filename(datasets, \"DRfold_infer.py\")\n    drfold_script = _require_single(\"DRfold_infer.py\", drfold_candidates, stage=\"DRFOLD2\", where=f\"{SCRIPT_LOC}:drfold\")\n    drfold_root = drfold_script.parent\n\n    if not drfold_script.exists():\n        _die(\"DRFOLD2\", f\"{SCRIPT_LOC}:drfold\", \"DRfold_infer.py ausente\", 1, [str(drfold_script)])\n    drfold_work = run_root / \"drfold2\"\n    drfold_work.mkdir(parents=True, exist_ok=True)\n    for tid in fallback_ids:\n        seq_raw = targets_df.filter(pl.col(\"target_id\") == tid).get_column(\"sequence\").item()\n        seq = _normalize_seq(seq_raw)\n        fasta = drfold_work / f\"{tid}.fa\"\n        fasta.write_text(f\">{tid}\\n{seq}\\n\", encoding=\"utf-8\")\n        outdir = drfold_work / tid\n        outdir.mkdir(parents=True, exist_ok=False)\n        cmd = [sys.executable, str(drfold_script), str(fasta), str(outdir)]\n        print(\"[RUN]\", \" \".join(cmd))\n        proc = subprocess.run(cmd, text=True, capture_output=True, cwd=str(drfold_root))\n        if proc.returncode != 0:\n            _die(\"DRFOLD2\", f\"{SCRIPT_LOC}:drfold\", \"runner falhou\", proc.returncode, _tail(proc.stdout, proc.stderr, 20))\n        pdb = outdir / \"relax\" / \"model_1.pdb\"\n        if not pdb.exists():\n            _die(\"DRFOLD2\", f\"{SCRIPT_LOC}:drfold\", \"model_1.pdb ausente\", 1, [tid, str(pdb)])\n        coords = _parse_c1prime_coords(pdb, target_id=tid, seq=seq)\n        rows = []\n        for model_id in range(1, int(N_MODELS) + 1):\n            for resid, resname, x, y, z in coords:\n                rows.append({\"target_id\": tid, \"model_id\": model_id, \"resid\": resid, \"resname\": resname, \"x\": x, \"y\": y, \"z\": z})\n        part_path = drfold_work / f\"{tid}_pred.parquet\"\n        pl.DataFrame(rows).write_parquet(part_path)\n        drfold_parts.append(part_path)\n\nif drfold_parts:\n    pl.concat([pl.read_parquet(p) for p in drfold_parts], how=\"vertical_relaxed\").write_parquet(drfold_path)\n    combined = pl.concat([pl.read_parquet(tbm_path), pl.read_parquet(drfold_path)], how=\"vertical_relaxed\")\nelse:\n    combined = pl.read_parquet(tbm_path)\ncombined.write_parquet(combined_path)\n\n_run([sys.executable, \"-m\", \"rna3d_local\", \"export-submission\", \"--sample\", str(sample_path), \"--predictions\", str(combined_path), \"--out\", str(submission_path)], env, repo_root)\n_run([sys.executable, \"-m\", \"rna3d_local\", \"check-submission\", \"--sample\", str(sample_path), \"--submission\", str(submission_path)], env, repo_root)\n\nsha = hashlib.sha256(submission_path.read_bytes()).hexdigest()\nprint(f\"[DONE] [{SCRIPT_LOC}] submission={submission_path}\")\nprint(f\"[INFO] [{SCRIPT_LOC}] submission_sha256={sha}\")\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
