from __future__ import annotations

import argparse

from .cli_commands import (
    _cmd_calibrate_kaggle_local,
    _cmd_evaluate_robust,
    _cmd_evaluate_submit_readiness,
    _cmd_evaluate_train_gate,
    _cmd_kaggle_submissions,
    _cmd_submit_kaggle,
)
from .cli_parser_args import add_calibration_history_gate_args, add_calibration_report_args, add_training_overfit_gate_args


def register_gating_parsers(sp: argparse._SubParsersAction[argparse.ArgumentParser]) -> None:
    sk = sp.add_parser("submit-kaggle", help="Notebook-only Kaggle submit after strict local + calibrated gating")
    sk.add_argument("--competition", default="stanford-rna-3d-folding-2")
    sk.add_argument("--sample", default="input/stanford-rna-3d-folding-2/sample_submission.csv")
    sk.add_argument("--submission", required=True, help="Local submission.csv used for strict validation/gating before submit")
    sk.add_argument("--predictions-long", default=None, help="Optional long predictions used to build submission; when provided, blocks branch=ensemble")
    sk.add_argument("--notebook-ref", required=True, help="Kaggle notebook ref owner/notebook-slug")
    sk.add_argument("--notebook-version", type=int, required=True, help="Notebook version number to submit")
    sk.add_argument("--notebook-file", default=None, help="File name generated inside notebook output (default: basename of --submission)")
    sk.add_argument("--message", required=True)
    sk.add_argument("--gating-report", default="runs/auto")
    sk.add_argument("--calibration-report", default="runs/auto")
    sk.add_argument("--robust-report", required=True, help="Robust evaluation report path (required)")
    sk.add_argument(
        "--readiness-report",
        required=True,
        help="Submit readiness report generated by evaluate-submit-readiness (required)",
    )
    sk.add_argument("--score-json", default=None)
    sk.add_argument("--baseline-score", type=float, default=None)
    sk.add_argument("--min-improvement", type=float, default=0.0, help="Minimum strict improvement required over baseline-score")
    sk.add_argument(
        "--baseline-public-score",
        type=float,
        default=None,
        help="If provided, enables calibrated gate using Kaggle submission history (requires --score-json)",
    )
    add_calibration_history_gate_args(sk)
    sk.add_argument("--require-min-cv-count", type=int, default=3)
    sk.add_argument("--is-smoke", action="store_true")
    sk.add_argument("--is-partial", action="store_true")
    sk.set_defaults(fn=_cmd_submit_kaggle)

    kc = sp.add_parser("calibrate-kaggle-local", help="Build local-vs-Kaggle public calibration report from submission history")
    kc.add_argument("--competition", default="stanford-rna-3d-folding-2")
    kc.add_argument("--out", default="runs/kaggle_calibration/latest.json")
    kc.add_argument("--page-size", type=int, default=100)
    kc.add_argument("--local-score", type=float, default=None, help="Optional candidate local score to estimate expected public range")
    kc.add_argument("--baseline-public-score", type=float, default=None, help="Optional baseline public score for a strict calibrated go/no-go decision")
    add_calibration_report_args(kc)
    kc.set_defaults(fn=_cmd_calibrate_kaggle_local)

    ks = sp.add_parser("kaggle-submissions", help="List competition submissions via KaggleApi (bypass broken kaggle CLI subcommand)")
    ks.add_argument("--competition", default="stanford-rna-3d-folding-2")
    ks.add_argument("--page-size", type=int, default=50)
    ks.add_argument("--page-token", default="")
    ks.add_argument("--out", default=None, help="Optional output JSON path")
    ks.set_defaults(fn=_cmd_kaggle_submissions)

    rb = sp.add_parser("evaluate-robust", help="Aggregate multiple local score.json files and apply robust + calibrated go/no-go gate")
    rb.add_argument(
        "--score",
        action="append",
        required=True,
        help="Named score entry in the format name=path/to/score.json. Use prefix cv: for CV splits (e.g., cv:fold0=...).",
    )
    rb.add_argument("--out", default="runs/auto")
    rb.add_argument("--predictions-long", default=None, help="Optional long predictions used by candidate; blocks branch=ensemble")
    rb.add_argument("--public-score-name", default="public_validation")
    rb.add_argument("--baseline-robust-score", type=float, default=None)
    rb.add_argument("--min-robust-improvement", type=float, default=0.0)
    rb.add_argument("--competition", default="stanford-rna-3d-folding-2")
    rb.add_argument("--baseline-public-score", type=float, default=None)
    add_calibration_history_gate_args(rb)
    rb.add_argument("--min-cv-count", type=int, default=2)
    rb.set_defaults(fn=_cmd_evaluate_robust)

    sr = sp.add_parser(
        "evaluate-submit-readiness",
        help="Evaluate complete pre-submit readiness (CV stability + strict improvements + local->Kaggle calibration)",
    )
    sr.add_argument(
        "--candidate-score",
        action="append",
        required=True,
        help="Candidate named score entry in the format name=path/to/score.json. Use prefix cv: for CV splits.",
    )
    sr.add_argument(
        "--baseline-score",
        action="append",
        default=[],
        help="Baseline named score entry in the format name=path/to/score.json. Must match candidate fold names for fold-level checks.",
    )
    sr.add_argument("--out", default="runs/auto")
    sr.add_argument("--predictions-long", default=None, help="Optional long predictions used by candidate; blocks branch=ensemble")
    sr.add_argument("--public-score-name", default="public_validation")
    sr.add_argument("--min-cv-count", type=int, default=3)
    sr.add_argument("--min-cv-improvement-count", type=int, default=2)
    sr.add_argument("--min-fold-improvement", type=float, default=0.0)
    sr.add_argument("--max-cv-regression", type=float, default=0.0)
    sr.add_argument("--min-robust-improvement", type=float, default=0.0)
    sr.add_argument("--min-public-local-improvement", type=float, default=0.0)
    sr.add_argument("--max-cv-std", type=float, default=0.03)
    sr.add_argument("--max-cv-gap", type=float, default=0.08)
    sr.add_argument("--competition", default="stanford-rna-3d-folding-2")
    sr.add_argument("--baseline-public-score", type=float, default=None)
    add_calibration_history_gate_args(sr)
    sr.add_argument("--min-calibration-pearson", type=float, default=0.0)
    sr.add_argument("--min-calibration-spearman", type=float, default=0.0)
    sr.set_defaults(fn=_cmd_evaluate_submit_readiness)

    tg = sp.add_parser("evaluate-train-gate", help="Evaluate anti-overfitting gate from trained QA model JSON (train_metrics vs val_metrics)")
    tg.add_argument("--model", required=True, help="qa_model.json or qa_gnn_model.json")
    tg.add_argument("--out", default="runs/auto")
    add_training_overfit_gate_args(tg)
    tg.set_defaults(fn=_cmd_evaluate_train_gate)
